{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mntcephfs/lab_data/hanyizhou/anaconda/pe/lib/python3.11/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/mntcephfs/lab_data/hanyizhou/anaconda/pe/lib/python3.11/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905969073/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2PreTrainedModel\n",
    "from torch import nn\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要修改的话应该修改两个部分, 第一个是gpt2本身的positional embedding, 一个是attention中的内容. 注意也许可以利用gpt2中的position_id这个信息."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention的输入是一句话, 即[batch, seq_length, hidden_dim], 输出和输入维度相同(如果不用cache).\n",
    "### Block的输入和attention的输入相同, 但是输出是hidden_state, 主要是attention_output+residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pe import FIRE\n",
    "#fire的输入是[batch, num_head, seq_length, hidden_dim]\n",
    "class GPT2Attention_Fire(nn.Module):\n",
    "    def __init__(self, config, is_cross_attention=False, layer_idx=None):\n",
    "        super().__init__()\n",
    "\n",
    "        max_positions = config.max_position_embeddings\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(\n",
    "                1, 1, max_positions, max_positions\n",
    "            ),\n",
    "            persistent=False,\n",
    "        )\n",
    "        self.register_buffer(\"masked_bias\", torch.tensor(-1e4), persistent=False)\n",
    "\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.split_size = self.embed_dim\n",
    "        if self.head_dim * self.num_heads != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"`embed_dim` must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n",
    "                f\" {self.num_heads}).\"\n",
    "            )\n",
    "\n",
    "        self.scale_attn_weights = config.scale_attn_weights\n",
    "        self.is_cross_attention = is_cross_attention\n",
    "\n",
    "        # Layer-wise attention scaling, reordering, and upcasting\n",
    "        self.scale_attn_by_inverse_layer_idx = config.scale_attn_by_inverse_layer_idx\n",
    "        self.layer_idx = layer_idx\n",
    "        self.reorder_and_upcast_attn = config.reorder_and_upcast_attn\n",
    "\n",
    "        if self.is_cross_attention:\n",
    "            self.c_attn = Conv1D(2 * self.embed_dim, self.embed_dim)\n",
    "            self.q_attn = Conv1D(self.embed_dim, self.embed_dim)\n",
    "        else:\n",
    "            self.c_attn = Conv1D(3 * self.embed_dim, self.embed_dim)\n",
    "        self.c_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "        self.pruned_heads = set()\n",
    "        self.fire = FIRE(num_heads=config.n_head)\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(heads, self.num_heads, self.head_dim, self.pruned_heads)\n",
    "        index_attn = torch.cat([index, index + self.split_size, index + (2 * self.split_size)])\n",
    "\n",
    "        # Prune conv1d layers\n",
    "        self.c_attn = prune_conv1d_layer(self.c_attn, index_attn, dim=1)\n",
    "        self.c_proj = prune_conv1d_layer(self.c_proj, index, dim=0)\n",
    "\n",
    "        # Update hyper params\n",
    "        self.split_size = (self.split_size // self.num_heads) * (self.num_heads - len(heads))\n",
    "        self.num_heads = self.num_heads - len(heads)\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
    "\n",
    "        if self.scale_attn_weights:\n",
    "            attn_weights = attn_weights / torch.full(\n",
    "                [], value.size(-1) ** 0.5, dtype=attn_weights.dtype, device=attn_weights.device\n",
    "            )\n",
    "\n",
    "        # Layer-wise attention scaling\n",
    "        if self.scale_attn_by_inverse_layer_idx:\n",
    "            attn_weights = attn_weights / float(self.layer_idx + 1)\n",
    "\n",
    "        if not self.is_cross_attention:\n",
    "            # if only \"normal\" attention layer implements causal mask\n",
    "            query_length, key_length = query.size(-2), key.size(-2)\n",
    "            #输出的attention矩阵是query×key\n",
    "            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "            mask_value = torch.finfo(attn_weights.dtype).min\n",
    "            # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
    "            # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
    "            mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n",
    "            attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n",
    "            \n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask\n",
    "            print(attention_mask)\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "        fire_bias = self.fire(query_length = query_length, key_length = key_length, device = query.device)\n",
    "        attn_weights += fire_bias\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n",
    "        attn_weights = attn_weights.type(value.dtype)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attn_weights = attn_weights * head_mask\n",
    "        \n",
    "        attn_output = torch.matmul(attn_weights, value)\n",
    "\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "    def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        # Use `torch.baddbmm` (a bit more efficient w/ alpha param for scaling -- from Megatron-LM)\n",
    "        bsz, num_heads, q_seq_len, dk = query.size()\n",
    "        _, _, k_seq_len, _ = key.size()\n",
    "\n",
    "        # Preallocate attn_weights for `baddbmm`\n",
    "        attn_weights = torch.empty(bsz * num_heads, q_seq_len, k_seq_len, dtype=torch.float32, device=query.device)\n",
    "\n",
    "        # Compute Scale Factor\n",
    "        scale_factor = 1.0\n",
    "        if self.scale_attn_weights:\n",
    "            scale_factor /= float(value.size(-1)) ** 0.5\n",
    "\n",
    "        if self.scale_attn_by_inverse_layer_idx:\n",
    "            scale_factor /= float(self.layer_idx + 1)\n",
    "\n",
    "        # Upcast (turn off autocast) and reorder (Scale K by 1 / root(dk))\n",
    "        with autocast(enabled=False):\n",
    "            q, k = query.reshape(-1, q_seq_len, dk), key.transpose(-1, -2).reshape(-1, dk, k_seq_len)\n",
    "            attn_weights = torch.baddbmm(attn_weights, q.float(), k.float(), beta=0, alpha=scale_factor)\n",
    "            attn_weights = attn_weights.reshape(bsz, num_heads, q_seq_len, k_seq_len)\n",
    "\n",
    "        if not self.is_cross_attention:\n",
    "            # if only \"normal\" attention layer implements causal mask\n",
    "            query_length, key_length = query.size(-2), key.size(-2)\n",
    "            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "            mask_value = torch.finfo(attn_weights.dtype).min\n",
    "            # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
    "            # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
    "            mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\n",
    "            attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op if otherwise\n",
    "        if attn_weights.dtype != torch.float32:\n",
    "            raise RuntimeError(\"Error with upcasting, attn_weights does not have dtype torch.float32\")\n",
    "        attn_weights = attn_weights.type(value.dtype)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attn_weights = attn_weights * head_mask\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value)\n",
    "\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "    def _split_heads(self, tensor, num_heads, attn_head_size):\n",
    "        \"\"\"\n",
    "        Splits hidden_size dim into attn_head_size and num_heads\n",
    "        \"\"\"\n",
    "        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n",
    "        tensor = tensor.view(new_shape)\n",
    "        return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
    "\n",
    "    def _merge_heads(self, tensor, num_heads, attn_head_size):\n",
    "        \"\"\"\n",
    "        Merges attn_head_size dim and num_attn_heads dim into hidden_size\n",
    "        \"\"\"\n",
    "        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)\n",
    "        return tensor.view(new_shape)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n",
    "        if encoder_hidden_states is not None:\n",
    "            if not hasattr(self, \"q_attn\"):\n",
    "                raise ValueError(\n",
    "                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n",
    "                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n",
    "                )\n",
    "\n",
    "            query = self.q_attn(hidden_states)\n",
    "            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n",
    "            attention_mask = encoder_attention_mask\n",
    "        else:\n",
    "            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n",
    "            '''在这一步拆出来key, value和query, 接下来才计算attention matrix以及新生成的token(即最终输出的a)'''\n",
    "            \n",
    "        #下面是对query等进行head切割, 可以使用self._split_heads进行切割\n",
    "        query = self._split_heads(query, self.num_heads, self.head_dim)\n",
    "        key = self._split_heads(key, self.num_heads, self.head_dim)\n",
    "        value = self._split_heads(value, self.num_heads, self.head_dim)\n",
    "\n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = layer_past\n",
    "            key = torch.cat((past_key, key), dim=-2)\n",
    "            value = torch.cat((past_value, value), dim=-2)\n",
    "\n",
    "        if use_cache is True:\n",
    "            present = (key, value)\n",
    "        else:\n",
    "            present = None\n",
    "\n",
    "        if self.reorder_and_upcast_attn:\n",
    "            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n",
    "        else:\n",
    "            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
    "            \n",
    "        '''\n",
    "        输入维度为(2,5,768), 2个batch, 5个token, hidden_dim=768\n",
    "        print(\"attn_output.shape\",attn_output.shape)\n",
    "        print(\"attn_weigthts.shape\", attn_weights.shape)\n",
    "        \n",
    "        attn_output.shape torch.Size([2, 12, 5, 64])\n",
    "        attn_weigthts.shape torch.Size([2, 12, 5, 5])\n",
    "        '''\n",
    "        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n",
    "        attn_output = self.c_proj(attn_output)\n",
    "        attn_output = self.resid_dropout(attn_output)\n",
    "\n",
    "        outputs = (attn_output, present)\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs  # a, (if use cache: present else None), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self, intermediate_size, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        self.c_fc = Conv1D(intermediate_size, embed_dim)\n",
    "        self.c_proj = Conv1D(embed_dim, intermediate_size)\n",
    "        self.act = ACT2FN[config.activation_function]\n",
    "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "    def forward(self, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatTensor:\n",
    "        hidden_states = self.c_fc(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.c_proj(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        return hidden_states # type: ignore\n",
    "class GPT2Block_Fire(nn.Module):\n",
    "    def __init__(self, config, layer_idx=None):\n",
    "        super().__init__()\n",
    "        hidden_size = config.hidden_size\n",
    "        inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.attn = GPT2Attention_Fire(config, layer_idx=layer_idx)\n",
    "        self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        if config.add_cross_attention:\n",
    "            self.crossattention = GPT2Attention_Fire(config, is_cross_attention=True, layer_idx=layer_idx)\n",
    "            self.ln_cross_attn = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        self.mlp = GPT2MLP(inner_dim, config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_1(hidden_states)\n",
    "        attn_outputs = self.attn(\n",
    "            hidden_states,\n",
    "            layer_past=layer_past,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
    "        outputs = attn_outputs[1:]\n",
    "        # residual connection\n",
    "        hidden_states = attn_output + residual\n",
    "\n",
    "        if encoder_hidden_states is not None:\n",
    "            # add one self-attention block for cross-attention\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n",
    "                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "            residual = hidden_states\n",
    "            hidden_states = self.ln_cross_attn(hidden_states)\n",
    "            cross_attn_outputs = self.crossattention(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            attn_output = cross_attn_outputs[0]\n",
    "            # residual connection\n",
    "            hidden_states = residual + attn_output\n",
    "            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        # residual connection\n",
    "        hidden_states = residual + feed_forward_hidden_states\n",
    "\n",
    "        if use_cache:\n",
    "            outputs = (hidden_states,) + outputs\n",
    "        else:\n",
    "            outputs = (hidden_states,) + outputs[1:]\n",
    "\n",
    "        return outputs  # hidden_states, (if use cache:  present), (attentions, cross_attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 写cope的结构\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2_modified import GPT2Attention_CoPE\n",
    "class GPT2Block_CoPE(nn.Module):\n",
    "    def __init__(self, config, layer_idx=None):\n",
    "        super().__init__()\n",
    "        hidden_size = config.hidden_size\n",
    "        inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.attn = GPT2Attention_CoPE(config, layer_idx=layer_idx)\n",
    "        self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        if config.add_cross_attention:\n",
    "            self.crossattention = GPT2Attention_Fire(config, is_cross_attention=True, layer_idx=layer_idx)\n",
    "            self.ln_cross_attn = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        self.mlp = GPT2MLP(inner_dim, config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_1(hidden_states)\n",
    "        attn_outputs = self.attn(\n",
    "            hidden_states,\n",
    "            layer_past=layer_past,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
    "        outputs = attn_outputs[1:]\n",
    "        # residual connection\n",
    "        hidden_states = attn_output + residual\n",
    "\n",
    "        if encoder_hidden_states is not None:\n",
    "            # add one self-attention block for cross-attention\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n",
    "                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "            residual = hidden_states\n",
    "            hidden_states = self.ln_cross_attn(hidden_states)\n",
    "            cross_attn_outputs = self.crossattention(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            attn_output = cross_attn_outputs[0]\n",
    "            # residual connection\n",
    "            hidden_states = residual + attn_output\n",
    "            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        # residual connection\n",
    "        hidden_states = residual + feed_forward_hidden_states\n",
    "\n",
    "        if use_cache:\n",
    "            outputs = (hidden_states,) + outputs\n",
    "        else:\n",
    "            outputs = (hidden_states,) + outputs[1:]\n",
    "\n",
    "        return outputs  # hidden_states, (if use cache:  present), (attentions, cross_attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试gpt2attention的输出结构\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Config\n",
    "config = GPT2Config()\n",
    "config.output_attentions = True\n",
    "\n",
    "model =GPT2Block_CoPE(config)\n",
    "\n",
    "x = torch.randn(size=[2,1,768])\n",
    "past = (torch.randn(size=[2,12,4,64]),torch.randn(size=[2,12,4,64]))\n",
    "model(x,layer_past=past, output_attentions=True)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: ln_1.weight shape: torch.Size([768]) is_grad: True\n",
      "name: ln_1.bias shape: torch.Size([768]) is_grad: True\n",
      "name: attn.c_attn.weight shape: torch.Size([768, 2304]) is_grad: True\n",
      "name: attn.c_attn.bias shape: torch.Size([2304]) is_grad: True\n",
      "name: attn.c_proj.weight shape: torch.Size([768, 768]) is_grad: True\n",
      "name: attn.c_proj.bias shape: torch.Size([768]) is_grad: True\n",
      "name: attn.cope.pos_emb shape: torch.Size([1, 1024, 768]) is_grad: True\n",
      "name: ln_2.weight shape: torch.Size([768]) is_grad: True\n",
      "name: ln_2.bias shape: torch.Size([768]) is_grad: True\n",
      "name: mlp.c_fc.weight shape: torch.Size([768, 3072]) is_grad: True\n",
      "name: mlp.c_fc.bias shape: torch.Size([3072]) is_grad: True\n",
      "name: mlp.c_proj.weight shape: torch.Size([3072, 768]) is_grad: True\n",
      "name: mlp.c_proj.bias shape: torch.Size([768]) is_grad: True\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print('name:',param[0], 'shape:', param[1].shape, 'is_grad:',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改GPT2中的attention layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel(config)\n",
    "h_fire = nn.ModuleList([GPT2Block_Fire(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n",
    "model.transformer.h = h_fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.named_parameters():\n",
    "    print('name:',param[0], 'shape:', param[1].shape, 'is_grad:',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 冻结模gpt2中wpe的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model.transformer.wpe.weight = torch.nn.Parameter(torch.zeros(model.transformer.wpe.weight.shape))\n",
    "model.transformer.wpe.weight.requires_grad = False\n",
    "for param in model.named_parameters():\n",
    "    if not param[1].requires_grad:\n",
    "        print('name:',param[0], 'shape:', param[1].shape, 'is_grad:',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调试GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mntcephfs/lab_data/hanyizhou/anaconda/pe/lib/python3.11/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/mntcephfs/lab_data/hanyizhou/anaconda/pe/lib/python3.11/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905969073/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[-0.4611, -0.9394, -0.3531,  ..., -1.0208,  0.4488,  0.0323],\n",
       "         [-0.6235, -0.0929, -0.9473,  ..., -0.4923,  0.3221, -0.1741],\n",
       "         [ 0.0394, -0.5655, -0.8255,  ..., -0.7238,  0.0981, -0.3650]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[ 5.0748e-01,  1.7871e-01,  9.7153e-01,  ..., -1.2899e+00,\n",
       "            5.4018e-02, -2.5398e-01],\n",
       "          [-9.8299e-01,  7.3517e-01, -6.0888e-01,  ..., -1.2616e-01,\n",
       "           -4.0250e-01,  9.6687e-01],\n",
       "          [ 3.4443e-01,  1.0429e-01, -1.4869e+00,  ...,  4.9544e-01,\n",
       "            1.3308e+00,  4.0614e-01]],\n",
       "\n",
       "         [[ 5.5370e-01, -6.1955e-02,  9.2159e-02,  ..., -1.2762e+00,\n",
       "           -1.4676e+00,  4.9077e-02],\n",
       "          [ 1.5382e-01,  1.8298e-01,  3.5432e-01,  ..., -6.0788e-01,\n",
       "           -8.5978e-01,  4.1258e-01],\n",
       "          [ 4.3824e-01,  8.8882e-03, -7.5354e-02,  ..., -2.7098e-02,\n",
       "           -3.8176e-01,  8.3070e-02]],\n",
       "\n",
       "         [[ 6.1174e-01, -3.8707e-01,  6.4838e-01,  ..., -1.4642e-01,\n",
       "           -4.9847e-01, -9.8351e-02],\n",
       "          [-1.2074e-01,  5.6025e-01,  9.5237e-02,  ...,  5.7150e-03,\n",
       "           -6.7081e-01,  3.3820e-01],\n",
       "          [ 9.9250e-01, -1.3065e-01, -2.5548e-01,  ..., -1.0058e+00,\n",
       "            7.9613e-01, -1.6853e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.3762e-01, -1.1814e-01,  8.2314e-01,  ..., -7.0227e-01,\n",
       "           -1.4630e-01, -1.1212e+00],\n",
       "          [ 1.0505e-01,  4.7243e-01, -9.9094e-01,  ..., -5.2147e-01,\n",
       "           -6.3302e-01, -1.1834e-03],\n",
       "          [-1.4457e-01, -4.3958e-01, -9.5484e-02,  ...,  1.5731e-02,\n",
       "            6.4962e-01, -2.3665e-01]],\n",
       "\n",
       "         [[ 5.5757e-01,  4.7529e-01, -1.7339e-01,  ...,  6.2240e-01,\n",
       "            5.8909e-01,  6.6990e-01],\n",
       "          [-1.6264e-01,  9.1058e-01,  4.5143e-01,  ...,  9.6360e-01,\n",
       "           -4.9983e-01,  1.5522e-01],\n",
       "          [ 1.4659e-01, -4.4021e-01,  2.6237e-03,  ..., -5.8681e-02,\n",
       "            8.9291e-01, -4.9215e-01]],\n",
       "\n",
       "         [[ 6.4398e-02, -3.3494e-01,  4.1272e-01,  ...,  6.0638e-01,\n",
       "            7.4312e-01,  5.0622e-01],\n",
       "          [ 1.0336e+00,  4.1777e-02, -2.4358e-03,  ..., -3.7749e-02,\n",
       "            2.9264e-01, -3.8732e-01],\n",
       "          [ 4.7884e-01,  4.8632e-02, -6.2284e-01,  ...,  8.2027e-01,\n",
       "            6.5673e-02, -5.7270e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[-0.4790, -0.5561, -0.3230,  ...,  0.3401,  0.2813, -0.4232],\n",
       "          [ 0.5292,  0.2144,  0.2530,  ...,  0.0386,  0.4070, -1.0276],\n",
       "          [-0.1195,  0.1033, -0.3204,  ...,  0.8619, -0.0173, -0.2176]],\n",
       "\n",
       "         [[-0.5544,  0.2788,  0.8152,  ..., -0.0301,  0.2728,  0.4194],\n",
       "          [ 0.0093,  0.5951,  0.0550,  ..., -0.6486,  0.1973,  0.6301],\n",
       "          [ 0.0883,  0.2372, -0.3802,  ..., -0.6107,  0.1633,  0.3244]],\n",
       "\n",
       "         [[ 0.4334, -0.2271,  0.0763,  ..., -0.3682, -0.6302, -0.1195],\n",
       "          [ 0.2368, -0.1885,  0.3360,  ...,  0.3524,  0.3173, -0.1924],\n",
       "          [-0.6771,  0.2381,  0.3837,  ..., -0.9131,  1.1291, -0.1726]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1895,  0.5060,  0.2229,  ...,  0.0994, -0.9731,  0.9275],\n",
       "          [-0.7265, -0.2592,  0.3762,  ..., -0.7272,  0.1138,  1.0641],\n",
       "          [ 0.7652, -0.1129,  0.4027,  ...,  0.0491,  0.9683,  1.3928]],\n",
       "\n",
       "         [[ 0.2750, -0.5391,  0.5643,  ...,  0.2313,  0.2975,  0.4802],\n",
       "          [ 0.2380,  0.2221, -0.8042,  ..., -0.6803,  0.6518,  0.2729],\n",
       "          [-0.1904, -0.0264, -0.4326,  ..., -0.0105,  0.2510,  0.5378]],\n",
       "\n",
       "         [[ 0.4139, -0.1933,  0.0140,  ..., -0.0300, -0.9212, -1.5206],\n",
       "          [ 0.0921, -0.6818,  0.6144,  ...,  0.7906,  0.2731, -0.9366],\n",
       "          [ 0.9485,  0.1212, -0.7499,  ...,  0.1156, -0.4459, -0.0440]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-0.3227, -0.1288,  0.2043,  ...,  1.0084, -0.2712,  0.0212],\n",
       "          [-0.5063,  0.6275, -0.5917,  ...,  0.4146, -0.2933, -0.9811],\n",
       "          [ 0.0682, -0.2942, -0.4442,  ...,  1.1458,  0.0206, -0.4150]],\n",
       "\n",
       "         [[-0.3222, -0.6161,  1.0172,  ..., -1.0558, -0.2293,  0.3548],\n",
       "          [-0.6976, -0.1277,  1.6060,  ..., -0.6509, -0.5074, -0.3473],\n",
       "          [-0.0886, -0.3252,  0.5356,  ..., -0.3723, -0.4153,  0.1402]],\n",
       "\n",
       "         [[-0.0971,  0.4750,  0.8963,  ..., -0.7553, -0.5444,  0.8588],\n",
       "          [ 0.1023,  0.9064,  0.2592,  ..., -0.0294,  0.3240,  1.0231],\n",
       "          [ 0.6266,  0.9488,  0.2761,  ..., -0.4045,  0.3806,  1.3947]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.7228,  0.8947, -0.0962,  ..., -0.8998, -0.4013, -0.5997],\n",
       "          [ 0.8462,  0.4114, -0.2517,  ..., -0.6405, -0.7401, -0.7538],\n",
       "          [ 1.4031,  0.4909, -1.0365,  ..., -1.0389, -0.5433, -0.0381]],\n",
       "\n",
       "         [[ 1.0237,  0.5707,  0.4152,  ..., -0.2266,  0.1866,  0.2447],\n",
       "          [ 1.5530,  0.5807, -0.2815,  ...,  0.0473, -0.4966,  0.1689],\n",
       "          [ 0.6957,  0.7047, -0.3237,  ..., -0.5219,  0.0051,  0.3487]],\n",
       "\n",
       "         [[-0.1424, -0.1541,  0.2125,  ...,  0.4501,  0.4198,  0.2481],\n",
       "          [-0.7753,  0.3384,  0.9014,  ..., -0.6568, -0.1755, -0.3169],\n",
       "          [-0.6807, -0.4363,  0.5466,  ...,  0.0276,  0.6310, -0.2120]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.7034, -0.0745, -0.8284,  ..., -0.1199, -0.3897,  0.1920],\n",
       "          [ 0.9720, -0.6779, -1.2226,  ...,  0.0324, -0.9156,  0.2917],\n",
       "          [ 1.6626,  0.0990, -0.7540,  ...,  0.1294, -0.5234,  0.6378]],\n",
       "\n",
       "         [[-0.0931,  0.0113, -0.2006,  ...,  0.1695,  0.5733,  0.5524],\n",
       "          [ 0.0094, -0.2151, -0.0595,  ...,  0.8691, -0.0701,  0.4847],\n",
       "          [-0.1338, -0.3313,  0.3108,  ...,  1.3378,  0.1960, -0.0196]],\n",
       "\n",
       "         [[ 0.1191, -0.7285,  0.1819,  ...,  0.4349,  0.1110,  0.3561],\n",
       "          [ 0.5230, -0.0264, -0.2847,  ...,  0.2420,  0.3117,  0.6540],\n",
       "          [ 0.0417,  0.3888,  0.3493,  ..., -0.4835,  0.5682,  0.0603]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0506,  0.2526,  0.2012,  ...,  0.7541, -0.2902,  0.4186],\n",
       "          [ 0.5914,  0.6972, -0.3055,  ...,  1.0180,  0.2402,  0.7531],\n",
       "          [-0.3906,  0.2721,  0.4292,  ...,  0.7361,  0.0344,  0.2551]],\n",
       "\n",
       "         [[-0.1013, -0.2469,  0.4814,  ..., -0.5576,  0.2151, -0.4638],\n",
       "          [-0.3390, -0.2596,  0.1146,  ..., -1.1543, -0.1003, -0.2403],\n",
       "          [ 0.1524, -0.2816,  0.3546,  ..., -0.7486, -0.2873,  0.2010]],\n",
       "\n",
       "         [[ 0.0630, -0.8898,  0.5465,  ...,  0.8293, -0.4912,  0.4204],\n",
       "          [ 0.7848, -1.5051,  0.4219,  ...,  0.9592, -0.6242,  0.4134],\n",
       "          [ 0.6522, -1.9630,  0.1053,  ...,  0.6498, -0.5224, -0.0159]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 2.4449e-01,  1.1252e+00, -2.0167e-01,  ...,  6.9045e-01,\n",
       "            5.5515e-03, -2.0459e-01],\n",
       "          [-1.5768e-01,  9.4496e-01, -1.0531e-03,  ...,  8.4932e-02,\n",
       "           -1.2303e-01, -4.1558e-01],\n",
       "          [-1.2905e-01,  1.1444e+00, -4.3768e-01,  ...,  2.2049e-01,\n",
       "            1.3714e-01, -1.6328e-01]],\n",
       "\n",
       "         [[-5.3595e-01,  7.2877e-01, -5.1211e-01,  ...,  7.6251e-01,\n",
       "           -5.4797e-01, -4.4903e-01],\n",
       "          [ 2.0086e-01,  2.8851e-01, -5.6780e-01,  ..., -8.1504e-01,\n",
       "           -3.0785e-01, -2.4719e-01],\n",
       "          [-1.8250e-01,  4.1051e-01, -1.4049e-01,  ..., -2.8227e-01,\n",
       "           -3.8042e-01, -6.1346e-01]],\n",
       "\n",
       "         [[-6.6681e-01,  1.0804e+00,  8.3683e-01,  ..., -1.5569e-01,\n",
       "            9.4601e-01, -4.4989e-01],\n",
       "          [-9.2974e-02,  4.8337e-01, -3.8792e-01,  ..., -5.1093e-01,\n",
       "            4.6484e-01, -3.6317e-02],\n",
       "          [ 8.1103e-03,  9.9877e-01,  2.2316e-01,  ...,  4.4116e-02,\n",
       "            8.5046e-01, -1.4169e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.8681e-01, -7.6247e-01, -6.6056e-01,  ..., -9.9789e-01,\n",
       "            1.5755e-01, -4.4172e-01],\n",
       "          [ 6.7676e-01, -8.8864e-01, -6.2936e-01,  ...,  1.2715e-01,\n",
       "           -5.6683e-01, -7.1317e-01],\n",
       "          [ 1.8770e-01, -8.1904e-01, -6.4800e-02,  ...,  1.9126e-01,\n",
       "           -1.0051e+00, -1.4545e-02]],\n",
       "\n",
       "         [[-7.7697e-02,  5.7294e-01,  5.6556e-02,  ...,  1.4942e-01,\n",
       "            3.9572e-01, -6.8033e-01],\n",
       "          [ 6.9842e-02,  5.4498e-01,  2.6515e-01,  ...,  2.0777e-01,\n",
       "           -3.1622e-01, -2.0002e-01],\n",
       "          [ 3.6050e-01,  6.5282e-01,  4.9047e-01,  ...,  6.5322e-02,\n",
       "            5.6735e-01,  6.0220e-01]],\n",
       "\n",
       "         [[ 1.7014e-01, -1.0427e+00, -4.0314e-01,  ..., -8.3431e-01,\n",
       "           -2.4938e-01, -1.1268e+00],\n",
       "          [ 1.8926e-01, -1.4627e+00, -2.7910e-01,  ..., -4.4752e-01,\n",
       "           -9.6102e-02, -5.8042e-01],\n",
       "          [ 7.9728e-01, -1.1002e+00, -2.9688e-01,  ..., -7.6877e-01,\n",
       "           -1.2621e-01, -8.4391e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[-0.1657,  0.5040, -0.7046,  ..., -0.3496,  0.2365,  0.3717],\n",
       "          [ 0.2630, -0.2289, -0.1645,  ...,  0.0362,  0.1676,  0.3481],\n",
       "          [ 0.1861,  0.2199, -0.2738,  ...,  0.2862,  0.1988, -0.7560]],\n",
       "\n",
       "         [[ 0.3121, -0.0883, -0.1140,  ..., -0.0057, -0.4402,  0.7555],\n",
       "          [ 0.2340,  0.0806, -0.5736,  ..., -0.7301, -0.1360,  0.6514],\n",
       "          [-0.2176, -0.1947, -0.7856,  ..., -0.7173,  0.1780,  1.1678]],\n",
       "\n",
       "         [[ 0.3866,  1.4070, -0.2998,  ..., -0.1231,  0.9444, -0.0826],\n",
       "          [-0.0997,  0.5845, -0.1488,  ..., -0.4543,  0.8109,  0.3498],\n",
       "          [ 0.1150,  0.2483,  0.1004,  ..., -0.2737,  0.5223, -0.1207]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0026,  0.6730,  0.6613,  ...,  0.0722, -0.6723,  0.9453],\n",
       "          [ 0.1155,  0.2179, -0.7206,  ...,  0.3384,  0.1908, -1.0244],\n",
       "          [ 0.7549, -0.1944, -0.0798,  ..., -0.2678,  0.1666,  0.2386]],\n",
       "\n",
       "         [[-0.4710,  0.3734,  0.1901,  ...,  0.5970,  0.5673,  0.6621],\n",
       "          [-0.5480,  0.6628, -0.0858,  ...,  0.5389,  0.8279,  0.5827],\n",
       "          [-0.6405,  0.6157, -0.1904,  ...,  0.8047,  0.0339,  0.9399]],\n",
       "\n",
       "         [[-0.2103, -0.6690,  0.4693,  ...,  0.5980,  0.3512, -0.1339],\n",
       "          [-0.6523, -0.6312,  0.6621,  ...,  0.7543,  0.0930, -0.2439],\n",
       "          [-0.9561, -0.5529,  0.7418,  ...,  0.6643,  0.0108, -0.1155]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.7948, -0.2601,  0.4219,  ..., -0.5010,  0.7233, -0.0467],\n",
       "          [-0.2133, -0.1490,  0.5334,  ..., -0.5392,  0.4935,  0.2606],\n",
       "          [-0.2539, -1.1052, -0.1387,  ...,  0.1543,  0.2200, -0.0641]],\n",
       "\n",
       "         [[-0.6125, -0.3336,  0.1138,  ..., -0.2867,  1.1449,  0.4341],\n",
       "          [-1.8484, -0.6242, -0.3032,  ..., -0.1357,  0.8074, -0.1219],\n",
       "          [-0.7697, -0.8493,  0.0567,  ...,  0.3592,  1.4002, -0.2897]],\n",
       "\n",
       "         [[ 0.2783,  0.0212, -0.5209,  ..., -1.1198,  0.1526, -0.5819],\n",
       "          [-0.0378, -0.0657, -0.4850,  ..., -0.0658,  0.3823, -0.3836],\n",
       "          [ 0.0493, -0.2887, -0.3531,  ..., -0.6305, -0.0196, -0.3120]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1309, -0.0614,  0.5964,  ..., -0.2945,  0.4034, -0.0571],\n",
       "          [-0.4020,  0.5465,  0.6959,  ..., -0.2212, -0.0494,  0.9012],\n",
       "          [-0.2146, -0.4601,  0.1955,  ...,  0.4449,  0.1552,  0.8111]],\n",
       "\n",
       "         [[ 1.0241,  0.3717, -0.1054,  ...,  0.5663, -0.6131, -0.1578],\n",
       "          [ 0.6119,  0.6069, -0.2029,  ...,  0.9055, -0.1937, -0.5184],\n",
       "          [ 0.3472,  0.1731, -0.0909,  ...,  0.9270, -0.1145, -0.2692]],\n",
       "\n",
       "         [[ 0.4580, -0.0513, -0.6151,  ..., -0.4009,  0.6194, -0.0751],\n",
       "          [ 0.9420, -0.4350, -0.1706,  ..., -0.3613, -0.1169, -0.6980],\n",
       "          [ 1.0819,  0.3330, -0.7351,  ..., -0.0890, -0.1000, -0.4674]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-0.0983,  0.7515,  0.4390,  ..., -0.0401, -0.2567, -0.0415],\n",
       "          [ 0.5662, -0.3287, -0.3699,  ..., -0.6002, -0.0526, -0.5111],\n",
       "          [-0.0278,  0.8390, -0.1930,  ...,  0.2270, -0.0725, -0.7021]],\n",
       "\n",
       "         [[-0.2128, -0.6996, -0.5143,  ..., -0.1852, -0.2107, -0.8970],\n",
       "          [-0.0023, -0.4816, -0.1836,  ...,  0.0832,  0.6392, -1.1067],\n",
       "          [-0.0568, -0.9385, -0.1185,  ..., -0.2077, -0.2019, -1.0173]],\n",
       "\n",
       "         [[-0.2205, -0.4804,  0.3744,  ..., -1.0257,  0.0764, -0.2175],\n",
       "          [-0.9306,  1.1199, -0.4261,  ..., -1.1443, -0.4332, -0.1770],\n",
       "          [-0.6110,  0.8430,  0.4117,  ..., -0.1165, -0.4037, -0.3628]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.4191, -1.1875, -0.3330,  ...,  0.3287, -0.0198,  0.3369],\n",
       "          [-0.2022, -0.6432, -0.4374,  ..., -0.3897,  0.0994, -0.2903],\n",
       "          [-0.2899, -0.1019, -0.3562,  ..., -0.6391,  0.2500,  0.2984]],\n",
       "\n",
       "         [[ 0.3580,  1.0249,  0.1128,  ...,  0.9191, -0.2229, -0.5507],\n",
       "          [-0.0741,  0.6413, -0.0340,  ...,  0.4153, -0.9657, -0.1307],\n",
       "          [ 0.8155,  1.0957, -0.3191,  ...,  0.5855, -0.0030, -0.1558]],\n",
       "\n",
       "         [[ 0.5325, -0.0407,  0.2258,  ...,  0.1515,  1.0866,  0.7038],\n",
       "          [ 0.3502, -0.7278, -0.8492,  ..., -0.2544,  0.3444,  1.3575],\n",
       "          [ 0.4982, -0.3359, -0.2110,  ..., -0.7904,  0.0076,  1.2781]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-0.2204, -0.3936, -0.7549,  ..., -0.0354, -0.9077,  0.1320],\n",
       "          [-0.2444,  0.1133, -0.5452,  ..., -0.0245, -0.3566,  0.0836],\n",
       "          [-0.3481, -0.0679, -0.9036,  ...,  0.0667, -0.2403,  0.0832]],\n",
       "\n",
       "         [[ 0.3166, -1.0808,  0.1108,  ..., -0.0146, -0.1307, -0.1178],\n",
       "          [-0.2165, -1.3168, -0.9266,  ...,  0.0166, -0.5259,  0.2737],\n",
       "          [-0.3412, -1.0670, -0.5499,  ..., -0.8770,  0.3829,  0.2702]],\n",
       "\n",
       "         [[-0.7788, -0.4042, -0.8800,  ..., -1.5625, -0.0434,  0.1138],\n",
       "          [-0.6856, -0.1068, -0.8793,  ..., -0.2394, -0.1013,  0.3078],\n",
       "          [-1.1133, -0.1749, -0.4051,  ..., -0.6066,  0.1723,  0.0863]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0148,  0.9688, -0.1158,  ..., -0.2154, -0.1234,  0.2603],\n",
       "          [ 0.5527,  0.7927, -0.3673,  ..., -0.4409, -0.1080,  0.2181],\n",
       "          [ 0.6989, -0.4518, -0.0962,  ..., -0.1156, -0.4087,  0.3666]],\n",
       "\n",
       "         [[ 1.8382,  0.9491,  0.9773,  ...,  0.6132,  0.0019,  0.6611],\n",
       "          [ 0.2410,  0.6685,  0.5407,  ...,  0.9509,  0.1124, -0.0288],\n",
       "          [ 0.0270,  1.4271,  0.3040,  ...,  1.0283,  0.2091,  0.3258]],\n",
       "\n",
       "         [[ 0.3423, -0.4481,  0.2281,  ..., -0.1875,  0.6159, -0.3415],\n",
       "          [-0.0158, -0.1983,  0.1574,  ...,  0.2022,  0.2332,  0.0130],\n",
       "          [ 0.7528, -0.3789,  0.7274,  ...,  0.3803,  0.2932, -0.0542]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-0.9064,  0.2358,  0.2476,  ..., -0.9496, -0.0342,  0.4774],\n",
       "          [-1.0772,  0.3871,  0.5773,  ..., -0.0681,  0.1783,  0.9064],\n",
       "          [-1.7617, -0.0490, -0.1753,  ...,  1.4805, -0.9722,  1.0431]],\n",
       "\n",
       "         [[-0.1650, -0.2459,  0.7889,  ..., -0.7684,  0.1052,  0.0520],\n",
       "          [-0.1274,  0.7560,  0.0995,  ...,  0.8053,  0.5239,  0.5852],\n",
       "          [ 0.3037, -0.5515,  0.4173,  ...,  0.9350,  0.1302,  1.1627]],\n",
       "\n",
       "         [[-0.4484,  0.0027,  0.2008,  ...,  0.1812, -0.5560,  0.5028],\n",
       "          [-0.4790,  0.2657, -0.4505,  ..., -0.4939, -0.8950,  0.1581],\n",
       "          [-0.5441,  0.1478, -0.3798,  ..., -0.4320, -0.1281,  0.0037]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.3796, -0.7472,  0.8847,  ...,  1.1980,  0.0723,  0.4371],\n",
       "          [-0.1667, -0.4737,  1.9227,  ...,  0.5465, -0.5476,  0.3634],\n",
       "          [ 0.1767, -0.4536,  1.5130,  ...,  0.7447,  0.1959,  0.3950]],\n",
       "\n",
       "         [[-0.7694,  0.0047,  0.4713,  ...,  0.9820,  0.6744,  0.1762],\n",
       "          [-0.8314,  0.1506,  0.5114,  ...,  0.4818,  0.1570,  0.1564],\n",
       "          [-1.1168,  0.1238,  0.4879,  ...,  0.3954,  0.3610, -0.2095]],\n",
       "\n",
       "         [[-0.3652,  0.4740, -0.5102,  ...,  0.0984, -0.6164, -0.5168],\n",
       "          [-0.3875,  0.0600, -1.1422,  ..., -0.0032,  0.4361,  0.3889],\n",
       "          [-0.6600,  0.3554, -0.6254,  ..., -0.0962, -0.2367, -0.6641]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.1002,  1.0893,  0.4401,  ...,  0.1398, -0.3757, -0.4486],\n",
       "          [ 0.0892,  0.7714,  0.9004,  ...,  0.0736,  0.3282, -0.3453],\n",
       "          [-0.1838,  0.5230,  0.5211,  ..., -0.4750, -0.5711, -0.5301]],\n",
       "\n",
       "         [[ 0.0564,  1.1887, -0.4143,  ..., -0.3504, -0.0167, -0.6294],\n",
       "          [ 0.4667,  1.3608, -0.0757,  ..., -0.2608,  0.2901, -1.4440],\n",
       "          [-0.1891,  1.0020,  0.2473,  ..., -0.1448, -0.8822, -0.0780]],\n",
       "\n",
       "         [[-0.4387, -1.1598, -0.1703,  ..., -0.8825,  0.4246,  0.5755],\n",
       "          [ 0.0215, -0.6920,  0.4909,  ..., -0.4132,  0.9338,  0.2641],\n",
       "          [-0.4329, -0.1894,  0.2870,  ..., -0.1642,  0.1106,  0.6618]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.2337, -0.3994, -0.0580,  ...,  0.2615,  0.4067,  0.4020],\n",
       "          [-0.2463, -0.3766, -0.4509,  ...,  0.4450,  0.3676,  0.6394],\n",
       "          [ 0.1164, -0.4381, -0.3333,  ...,  0.4741, -0.0272,  0.0626]],\n",
       "\n",
       "         [[-0.1344,  0.5351, -0.2364,  ...,  0.4639,  0.0824,  0.4444],\n",
       "          [ 0.1408,  1.1044,  0.0781,  ...,  0.6510,  0.9711,  0.6698],\n",
       "          [ 0.0448,  1.0337,  0.3147,  ..., -0.3112,  0.7005,  0.7953]],\n",
       "\n",
       "         [[-0.6079, -0.8901,  0.6360,  ..., -0.0079,  0.4636, -0.1696],\n",
       "          [-0.7832, -0.4636,  0.7237,  ..., -0.1897,  0.3732, -0.2301],\n",
       "          [-0.2797, -0.3591,  0.4078,  ...,  0.1958,  0.3029,  0.4439]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.4921, -0.6314,  1.1877,  ...,  0.4595,  0.6735,  0.8268],\n",
       "          [ 0.3664, -0.4333,  0.5390,  ...,  0.4485,  1.0578,  1.2270],\n",
       "          [ 0.3347, -0.2557,  0.3696,  ..., -0.3846,  0.5896,  1.1103]],\n",
       "\n",
       "         [[-0.4461,  0.1959,  0.2201,  ..., -0.2478,  0.1953, -0.6392],\n",
       "          [-0.2534,  0.8041,  0.5894,  ...,  0.3280, -0.0705, -1.2555],\n",
       "          [-0.1373,  0.5095,  0.1966,  ..., -0.0924, -0.2233, -0.9649]],\n",
       "\n",
       "         [[ 1.1467,  0.2358, -1.1651,  ..., -0.0142,  0.3584, -0.0027],\n",
       "          [ 0.6471,  0.2687, -0.3082,  ..., -0.9493, -0.0540,  0.5090],\n",
       "          [ 0.1793,  0.5622, -0.1267,  ..., -0.2911, -0.5145,  0.2226]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.6578, -0.2288,  0.2818,  ..., -0.0921, -0.1762, -0.2712],\n",
       "          [-0.1522, -0.5442,  0.6927,  ...,  0.1694, -0.2406, -0.3629],\n",
       "          [-0.2090, -0.2256,  1.0044,  ...,  0.5129, -0.5525, -0.1586]],\n",
       "\n",
       "         [[-0.5005,  0.4476,  0.3668,  ..., -0.0075,  0.0969, -0.4913],\n",
       "          [-1.0542,  0.1176,  0.5885,  ..., -0.1355,  0.1806,  0.0101],\n",
       "          [-0.7602,  0.1924, -0.1626,  ...,  0.6321,  0.3646, -0.6793]],\n",
       "\n",
       "         [[ 0.1941,  0.2413,  0.0951,  ...,  0.3855,  0.0179,  0.8209],\n",
       "          [ 0.2420,  0.6845, -0.1390,  ...,  0.1587,  0.2990,  0.2708],\n",
       "          [ 0.3518,  0.4748,  0.0892,  ...,  0.0969,  0.4415, -0.0459]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-0.5375,  0.2625, -0.6014,  ...,  0.4431, -0.3416, -0.0629],\n",
       "          [-1.0420,  0.4723,  0.3345,  ..., -0.9059, -0.7835, -0.2160],\n",
       "          [-0.4105,  0.8899, -0.2032,  ..., -0.0238, -1.0919,  0.5268]],\n",
       "\n",
       "         [[ 0.1379,  1.0992, -0.6289,  ..., -0.1935, -0.1077, -0.0863],\n",
       "          [ 0.5322,  0.3508, -0.4228,  ...,  0.6255,  0.2185, -0.3116],\n",
       "          [ 0.7242,  0.6085, -0.7181,  ...,  0.6210,  0.0170, -0.3143]],\n",
       "\n",
       "         [[-0.2119,  0.2433, -0.2933,  ..., -0.3809,  0.3781, -0.6580],\n",
       "          [-0.3592, -0.0130, -0.5933,  ...,  0.2140, -0.2238, -0.5324],\n",
       "          [-0.0789, -0.0908, -0.9770,  ...,  0.6517, -0.0173, -1.6577]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1462, -0.3534, -1.5219,  ..., -0.3786,  0.6801, -0.5184],\n",
       "          [ 0.1560,  0.1969, -1.1622,  ...,  0.0466,  0.4588,  0.2166],\n",
       "          [ 0.4533,  0.8414, -0.7094,  ...,  0.6616,  0.6815, -0.2865]],\n",
       "\n",
       "         [[-0.0504, -0.6467, -0.4968,  ..., -0.5486, -0.0595,  0.3065],\n",
       "          [-0.4538,  0.5823,  0.0593,  ..., -0.1181, -1.1311, -0.2180],\n",
       "          [ 0.1133,  0.1355, -0.5212,  ..., -1.0306, -0.1533, -0.4230]],\n",
       "\n",
       "         [[ 0.1626, -1.2818, -0.0369,  ..., -0.3707,  0.7052, -0.8337],\n",
       "          [-0.5329, -1.0032,  0.0235,  ...,  0.6857,  0.9447, -1.1528],\n",
       "          [-1.2466, -0.9147, -0.6313,  ...,  0.1627,  0.8249, -1.1859]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.5055,  0.1111,  0.3326,  ...,  0.8176,  0.3318, -0.5421],\n",
       "          [-0.3580,  0.3215, -0.0998,  ...,  0.5368, -0.0512, -0.3127],\n",
       "          [-0.1173,  0.0409,  0.3668,  ..., -0.0275,  0.0512, -0.5390]],\n",
       "\n",
       "         [[-0.3111,  0.1525,  0.3897,  ...,  1.0640, -0.4026, -0.3033],\n",
       "          [-0.0890,  1.0278, -0.1872,  ...,  0.7006, -0.3684,  0.4185],\n",
       "          [-0.4444,  0.5259, -0.7957,  ...,  0.6306, -0.6471,  0.3038]],\n",
       "\n",
       "         [[-0.0090,  0.0605,  0.9052,  ...,  0.0238, -0.6117,  0.6865],\n",
       "          [ 0.2662,  0.0187,  0.5907,  ..., -0.4446, -0.7012,  0.5379],\n",
       "          [ 0.2396, -0.4464,  0.8165,  ..., -0.2776, -0.0093,  0.7892]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.5578,  0.6887,  0.3662,  ..., -0.1361,  0.0180,  0.3712],\n",
       "          [-0.7804,  0.7554, -0.2398,  ..., -0.0638, -0.4289,  0.8871],\n",
       "          [ 0.4981,  0.8522,  0.4120,  ..., -0.0433,  0.4556,  0.6530]],\n",
       "\n",
       "         [[ 0.0086,  0.1399, -0.3997,  ..., -0.8384, -0.1909,  0.7986],\n",
       "          [-0.2040,  0.0596, -1.0696,  ..., -0.0767, -0.3700,  0.3606],\n",
       "          [ 0.6777,  0.8550, -0.3036,  ..., -0.5820, -0.4460,  0.0664]],\n",
       "\n",
       "         [[-0.1331, -0.7509,  0.7962,  ..., -1.6717, -0.4057,  2.1635],\n",
       "          [ 0.3436,  0.2180,  0.4446,  ..., -0.1832,  0.0378,  1.5024],\n",
       "          [ 0.2274, -0.4111,  1.0127,  ..., -0.3397,  0.1798,  1.6701]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-4.7565e-01, -5.3001e-01, -3.0267e-01,  ..., -1.6525e-01,\n",
       "           -5.7499e-01, -1.9262e-01],\n",
       "          [-2.9195e-01,  1.0050e-01, -1.3619e-01,  ..., -2.4152e-01,\n",
       "           -4.4155e-01, -2.8138e-01],\n",
       "          [-7.4099e-01, -5.0066e-01, -1.0162e-01,  ..., -7.4677e-01,\n",
       "           -2.2865e-01, -2.5551e-01]],\n",
       "\n",
       "         [[ 1.8832e-01,  7.6060e-01,  1.2572e-01,  ...,  6.5730e-01,\n",
       "            4.7831e-01,  3.3668e-01],\n",
       "          [ 2.7361e-01,  3.4941e-01,  1.9788e-01,  ...,  6.4009e-01,\n",
       "           -4.7459e-01, -2.1002e-01],\n",
       "          [-2.0900e-01,  6.7564e-01,  4.1384e-02,  ...,  8.9757e-01,\n",
       "           -7.6224e-01,  3.2212e-01]],\n",
       "\n",
       "         [[ 1.1378e-01,  8.5265e-01,  3.2854e-01,  ..., -9.0045e-02,\n",
       "           -9.4593e-01,  4.4826e-01],\n",
       "          [-1.2671e-01,  8.5519e-01, -2.9302e-01,  ..., -1.9520e-01,\n",
       "           -3.9008e-01,  1.7851e-01],\n",
       "          [-2.0766e-01,  8.0204e-01, -1.8370e-01,  ..., -4.2710e-01,\n",
       "           -3.6757e-01,  1.7269e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.4297e-01, -1.0210e+00,  1.1784e-01,  ..., -9.1156e-01,\n",
       "           -8.2759e-01,  7.7392e-01],\n",
       "          [ 8.0366e-01, -6.6274e-01,  6.1180e-01,  ..., -5.1476e-01,\n",
       "           -5.3873e-01, -1.1870e-01],\n",
       "          [ 7.7309e-01, -8.2389e-01,  2.0169e-01,  ..., -5.6729e-01,\n",
       "           -4.2462e-01, -1.9003e-01]],\n",
       "\n",
       "         [[-3.7052e-02,  4.8910e-01, -3.1248e-01,  ..., -7.0339e-02,\n",
       "            6.9747e-01, -5.2617e-01],\n",
       "          [-3.2624e-01,  9.9155e-02,  5.6824e-04,  ..., -6.4991e-01,\n",
       "           -3.4470e-01, -2.2545e-01],\n",
       "          [-2.2512e-02,  4.2281e-01, -7.8572e-01,  ...,  4.1572e-01,\n",
       "            1.0777e+00,  5.5265e-01]],\n",
       "\n",
       "         [[-3.1204e-01,  7.1215e-01,  2.2649e-01,  ..., -2.2413e-01,\n",
       "           -7.7614e-01, -1.8530e-02],\n",
       "          [ 4.5736e-01, -1.3187e-01,  9.7869e-02,  ..., -7.3804e-01,\n",
       "           -3.8433e-01,  5.4836e-01],\n",
       "          [-4.9472e-02, -2.2628e-01,  3.2087e-01,  ..., -6.3118e-01,\n",
       "           -6.2849e-01,  9.2444e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[-1.1662e-01,  5.0948e-01, -2.7068e-01,  ..., -4.2964e-01,\n",
       "            5.2687e-01,  8.8183e-01],\n",
       "          [-5.0319e-01, -3.8813e-01, -9.9047e-01,  ...,  5.4747e-05,\n",
       "            6.4405e-02, -4.8752e-01],\n",
       "          [-2.9932e-01,  1.0348e-01, -1.2205e-01,  ...,  3.5223e-02,\n",
       "            1.9362e-01, -2.4114e-01]],\n",
       "\n",
       "         [[-9.5908e-01, -4.2993e-01, -2.0892e-01,  ..., -1.0231e-01,\n",
       "           -6.4677e-01, -3.1129e-01],\n",
       "          [-4.1445e-01, -2.3257e-01,  4.5589e-01,  ..., -6.1938e-01,\n",
       "           -7.3737e-01, -8.7385e-01],\n",
       "          [ 5.0863e-02,  2.2941e-02,  3.8417e-01,  ..., -6.0050e-01,\n",
       "            1.2516e-01, -9.3374e-01]],\n",
       "\n",
       "         [[-2.9968e-01, -1.6139e-01, -5.6452e-01,  ...,  4.9763e-01,\n",
       "            3.0288e-01, -8.8143e-01],\n",
       "          [ 9.4706e-03,  2.1230e-01,  1.2529e-01,  ...,  1.0088e+00,\n",
       "            1.0368e-01, -7.8202e-01],\n",
       "          [ 5.9116e-01, -3.8617e-01,  7.4624e-01,  ...,  9.1819e-01,\n",
       "            4.1554e-01, -1.0889e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0164e-01, -8.1551e-01,  3.8070e-01,  ..., -2.2514e-01,\n",
       "           -8.0825e-02,  2.9961e-01],\n",
       "          [-2.4271e-01, -2.1913e-01, -7.5083e-01,  ..., -1.9791e-01,\n",
       "           -1.1029e-01,  1.1852e-01],\n",
       "          [-3.4668e-01, -5.5644e-01, -1.8769e-01,  ..., -3.8627e-01,\n",
       "           -1.3480e-01,  4.7582e-01]],\n",
       "\n",
       "         [[ 6.2048e-01, -1.4584e+00, -7.7132e-01,  ...,  2.4568e-01,\n",
       "           -5.2250e-01,  9.5605e-02],\n",
       "          [-1.5751e-01, -1.0726e+00, -7.6170e-01,  ..., -7.0633e-03,\n",
       "           -9.4941e-01, -2.6044e-01],\n",
       "          [ 3.1093e-01, -8.4444e-01, -7.4647e-01,  ...,  3.3233e-01,\n",
       "           -4.2343e-01, -4.0277e-01]],\n",
       "\n",
       "         [[-2.5515e-01, -2.7048e-01,  5.9080e-01,  ..., -5.3944e-02,\n",
       "           -2.1551e-01,  9.2912e-01],\n",
       "          [-7.6774e-01,  7.2123e-03,  8.5145e-01,  ..., -2.4877e-03,\n",
       "           -1.0096e+00,  6.8647e-01],\n",
       "          [-1.1974e-01,  3.4771e-03, -6.6553e-02,  ...,  6.7512e-01,\n",
       "           -5.0968e-01,  5.3583e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.1105, -0.4738, -0.2376,  ...,  0.2178,  0.5600,  0.4898],\n",
       "          [ 0.1049,  0.1094, -0.6537,  ...,  0.1830,  0.8162,  0.7344],\n",
       "          [-0.3869, -0.3008, -0.8911,  ...,  0.0058,  0.1908,  0.3548]],\n",
       "\n",
       "         [[-1.6137, -0.3337, -0.6025,  ..., -0.5446,  0.3162,  1.4513],\n",
       "          [-1.6297, -0.5418, -1.0639,  ...,  0.2688,  0.3909,  1.3462],\n",
       "          [-1.2617,  0.3078, -0.2549,  ...,  0.1110,  0.5002,  1.2888]],\n",
       "\n",
       "         [[ 0.6277,  0.2725, -0.3860,  ...,  0.6050,  0.2557, -0.3048],\n",
       "          [ 0.5960,  0.1285, -0.5366,  ..., -0.5088,  0.2264,  0.3577],\n",
       "          [ 0.7659,  0.3383, -0.3146,  ..., -0.4198,  0.6180, -0.1964]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1942, -0.3253,  0.8110,  ...,  0.7591, -0.8804, -0.6349],\n",
       "          [ 0.5136,  0.7667,  1.3770,  ...,  0.0792, -0.5275, -1.0564],\n",
       "          [ 0.5525,  0.5723,  1.0897,  ..., -0.1196, -0.2207, -0.6780]],\n",
       "\n",
       "         [[ 0.3085, -1.0474, -0.3068,  ..., -0.3163, -0.1562, -1.3018],\n",
       "          [ 0.2837,  0.3453, -0.4693,  ...,  0.0628,  0.1135,  0.4709],\n",
       "          [-0.5515, -0.5682, -0.5264,  ..., -0.1310,  0.2390, -0.1828]],\n",
       "\n",
       "         [[ 0.0324, -0.4105,  0.6904,  ..., -0.2352, -0.0345,  0.2222],\n",
       "          [ 0.0572, -0.3234,  0.3396,  ...,  0.3816, -0.0088, -0.1590],\n",
       "          [ 0.1624, -0.5112,  0.2954,  ...,  0.5990,  0.7703,  0.2913]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-0.4086, -0.4081,  0.4509,  ...,  0.0660,  0.3841,  0.4033],\n",
       "          [-0.0070, -0.7027,  0.7202,  ...,  0.0239, -0.7504,  0.1023],\n",
       "          [-0.4900, -0.8032,  0.1759,  ..., -0.5294, -1.0416, -0.4397]],\n",
       "\n",
       "         [[-0.7429,  0.2489,  0.1589,  ..., -0.3187, -0.3006,  0.5547],\n",
       "          [ 0.1690, -0.4935, -0.3828,  ..., -0.6575,  0.0349,  0.5259],\n",
       "          [-0.4548,  0.3230,  0.0654,  ..., -0.9117,  0.1455,  0.6604]],\n",
       "\n",
       "         [[ 1.0266,  0.4812, -0.3349,  ...,  0.2300,  0.0188,  0.3393],\n",
       "          [-0.3021,  1.1245,  0.3199,  ...,  0.1947, -0.0594, -0.4695],\n",
       "          [ 0.8595,  0.3587,  0.0945,  ..., -0.0259,  0.0494, -0.3502]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1780,  1.1517,  0.3281,  ..., -0.1183,  0.0801, -0.4344],\n",
       "          [ 0.8622,  0.3350, -0.1059,  ...,  0.1804,  0.0111,  0.3081],\n",
       "          [-1.0788,  0.2588,  0.2022,  ...,  0.0886,  0.7513, -0.0755]],\n",
       "\n",
       "         [[ 0.2902, -0.0511,  0.8473,  ..., -0.4358, -0.4169, -0.4205],\n",
       "          [-0.7892, -0.7579,  0.5734,  ...,  0.0633, -0.2788,  0.0502],\n",
       "          [ 0.0838, -1.3780,  0.2977,  ..., -0.7040, -0.0203, -0.4573]],\n",
       "\n",
       "         [[-0.7147, -0.0842, -0.5142,  ...,  0.1154,  0.7353, -0.4267],\n",
       "          [-0.4787, -0.3060, -0.0573,  ...,  0.4355,  0.3025,  0.8390],\n",
       "          [ 0.2437, -0.6288, -0.3003,  ...,  0.1235, -0.0294, -0.2501]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.3976, -0.2884, -0.0656,  ...,  0.0852, -0.1447,  1.4223],\n",
       "          [-0.0287, -0.3127,  0.0178,  ..., -0.4121,  0.1823,  0.1378],\n",
       "          [ 0.5790, -0.1569, -0.1572,  ..., -0.1405, -0.0294,  0.7978]],\n",
       "\n",
       "         [[-0.6105,  0.0353,  0.5345,  ..., -0.7626,  0.1293, -0.3267],\n",
       "          [-0.7847,  0.1988,  0.3139,  ..., -0.5540,  0.5039, -0.0637],\n",
       "          [-0.3565,  0.2509, -0.1060,  ..., -0.0338,  0.2932,  0.3379]],\n",
       "\n",
       "         [[ 0.1616,  0.6304, -0.2124,  ..., -0.0356,  1.3803,  0.2810],\n",
       "          [-0.1446,  0.5856, -0.6513,  ...,  0.5618,  0.6484, -0.0291],\n",
       "          [-0.1107,  0.6381, -0.5568,  ...,  0.3188,  1.0073, -0.6769]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.8222,  0.0223, -1.0025,  ..., -0.0743, -0.8650,  0.0541],\n",
       "          [-0.3036,  0.3682, -0.3211,  ...,  0.1572, -0.4895,  0.3254],\n",
       "          [-0.6189, -0.2465, -1.0119,  ..., -0.4275, -0.3763,  0.2877]],\n",
       "\n",
       "         [[-0.1158,  0.0999, -0.2166,  ...,  0.4365, -1.2144, -0.1521],\n",
       "          [ 0.7988,  0.7281,  0.3507,  ..., -0.1180, -0.5809,  0.1270],\n",
       "          [ 0.5686,  0.7809,  0.9957,  ...,  0.0203, -0.5657,  0.0256]],\n",
       "\n",
       "         [[-0.4478,  0.3732, -0.9573,  ...,  0.4830,  0.3871, -0.2098],\n",
       "          [ 0.0484,  0.2160, -1.1678,  ..., -0.3044, -0.0777,  0.5812],\n",
       "          [-0.5703,  0.9465, -0.9272,  ...,  0.2791,  0.4394,  0.3293]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.8586, -1.2557,  0.4131,  ..., -0.8946, -0.2746, -0.2003],\n",
       "          [ 0.1131, -1.3626,  0.1392,  ...,  0.1757, -0.1823,  0.4648],\n",
       "          [ 0.7762, -0.7179, -0.2217,  ..., -0.3890,  0.1076, -0.0489]],\n",
       "\n",
       "         [[-0.6286, -0.2314, -0.4555,  ...,  0.4763, -0.0131,  0.8539],\n",
       "          [-0.2818, -0.8164, -0.0126,  ...,  0.7508,  0.3179,  0.2097],\n",
       "          [-0.2702, -0.5521,  0.4722,  ...,  0.3900,  0.5904,  0.1572]],\n",
       "\n",
       "         [[ 0.1257,  0.0986, -0.4353,  ..., -1.1002, -0.5713,  0.9138],\n",
       "          [-0.6603,  0.5970, -0.3562,  ...,  0.5828, -0.7536,  0.3323],\n",
       "          [-0.6795,  0.0217, -0.9348,  ...,  0.6335, -0.3432,  0.2717]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.5905, -0.4777, -0.6364,  ...,  0.0773, -1.0183, -0.1689],\n",
       "          [ 0.9225, -0.0516, -0.5292,  ..., -0.1741, -0.3690, -0.6175],\n",
       "          [ 0.3124, -0.2744, -0.5636,  ..., -0.3169, -0.2433, -0.1954]],\n",
       "\n",
       "         [[-0.4259,  0.5857,  0.0176,  ..., -0.7157, -0.2110,  0.7205],\n",
       "          [-1.0498,  0.1156, -0.1889,  ..., -0.7122,  0.0646,  0.2530],\n",
       "          [-1.0758,  0.1659, -0.4225,  ..., -0.2822,  0.2209,  0.8627]],\n",
       "\n",
       "         [[-1.0275, -0.1415,  0.6270,  ...,  0.8088,  0.4624, -0.0879],\n",
       "          [-0.5016, -0.2052,  0.3498,  ...,  0.3071, -0.3225,  0.0588],\n",
       "          [-0.5555, -0.1864,  0.1886,  ...,  0.2302, -0.1611,  0.3732]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-0.4062,  0.1721,  0.0296,  ..., -0.5240, -0.7262,  0.1071],\n",
       "          [-0.4047,  0.4001, -0.6024,  ..., -0.3470, -1.1765,  0.8353],\n",
       "          [ 0.0502, -0.0234, -0.7439,  ..., -0.7076, -0.2473,  1.1499]],\n",
       "\n",
       "         [[-0.1426,  0.7067, -0.3760,  ..., -0.2464,  0.0293, -1.0947],\n",
       "          [-0.5429, -0.0899, -0.1709,  ...,  0.7775,  0.4617, -0.6704],\n",
       "          [-0.8869,  1.0752, -0.1449,  ...,  0.4802,  0.6191, -0.8606]],\n",
       "\n",
       "         [[ 1.1290,  1.3575, -0.0229,  ..., -0.5527,  0.2236, -0.2998],\n",
       "          [ 0.6003,  1.2486,  0.1269,  ..., -0.6217,  0.7469, -0.1364],\n",
       "          [ 0.8176,  0.4761, -0.0576,  ..., -0.2542,  0.8187,  0.0786]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0131,  0.4237, -0.0098,  ...,  0.0842,  1.0380,  0.5001],\n",
       "          [-0.1897, -0.3133,  0.2653,  ...,  1.2271, -0.1151, -0.4674],\n",
       "          [ 0.1936, -0.2976, -0.2550,  ...,  0.4805,  0.1554, -0.1647]],\n",
       "\n",
       "         [[ 0.8220,  0.4855, -0.1149,  ...,  0.1139,  0.2070,  0.6352],\n",
       "          [ 0.8968,  0.3135, -0.3527,  ..., -0.1256, -0.4274,  0.1044],\n",
       "          [ 0.5156,  0.2607, -0.2390,  ..., -0.8754,  0.4509,  0.4323]],\n",
       "\n",
       "         [[ 0.6191, -0.5943, -0.2668,  ..., -0.1509, -0.1590, -0.2090],\n",
       "          [-0.2116, -0.5731,  0.2377,  ...,  0.0578, -0.2627,  0.1179],\n",
       "          [-0.0047,  0.0768, -0.2054,  ...,  0.3716,  0.0241,  0.7288]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-0.2471,  0.9895,  0.8054,  ..., -0.0076,  0.5224,  0.4882],\n",
       "          [-0.0152,  0.6405,  0.6017,  ..., -0.5910,  0.0507, -0.2700],\n",
       "          [ 0.1411,  0.8863,  0.4804,  ..., -1.1756, -0.0927, -0.5580]],\n",
       "\n",
       "         [[ 0.5922, -0.7708, -0.6766,  ..., -0.0197, -0.4002, -0.2546],\n",
       "          [-0.4225, -1.1012, -0.5005,  ..., -0.7808, -0.4937,  0.2086],\n",
       "          [-0.0202, -0.8767, -1.4214,  ..., -0.2632, -1.1514,  0.3693]],\n",
       "\n",
       "         [[-0.0150,  0.2397, -0.5111,  ...,  0.4903, -0.6641, -0.6730],\n",
       "          [ 0.1587,  0.3594, -0.2250,  ..., -0.1845, -1.0089, -0.9251],\n",
       "          [-0.1980,  0.0460, -0.3362,  ..., -0.2780, -1.2600, -0.7817]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0908,  0.1149, -0.6734,  ..., -0.9822,  0.8316,  0.4064],\n",
       "          [-0.0244,  0.7172, -0.0241,  ..., -0.6174, -0.0452, -0.0458],\n",
       "          [ 0.1060,  0.6240, -0.2137,  ..., -0.4460,  0.1160, -0.6238]],\n",
       "\n",
       "         [[-0.3117,  0.4003, -1.2391,  ..., -1.1661,  0.3449,  1.0319],\n",
       "          [ 0.6361,  1.1787, -1.1467,  ..., -0.4527, -0.1796,  0.0625],\n",
       "          [ 0.1322,  0.6255, -1.0569,  ..., -0.4343,  0.2811, -0.0895]],\n",
       "\n",
       "         [[ 0.0420, -0.7991,  0.4612,  ...,  1.1860,  0.0440,  0.2629],\n",
       "          [-0.0567, -0.3907, -0.0953,  ...,  0.4000,  0.6133,  0.2978],\n",
       "          [-0.6175, -0.4794,  0.5012,  ...,  0.3023,  0.5156, -0.1845]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.5678,  0.1053,  0.4490,  ...,  0.0509,  0.5264,  0.2577],\n",
       "          [ 0.4827,  0.1122,  0.2948,  ..., -0.5077, -0.0375,  0.4617],\n",
       "          [ 1.0164, -0.1618, -0.0317,  ..., -0.1464,  0.6557, -0.2566]],\n",
       "\n",
       "         [[-0.9700, -0.0538,  0.6338,  ..., -0.8658, -0.9060,  0.2531],\n",
       "          [-0.3350, -0.6239,  0.2257,  ..., -0.1007, -0.2978, -0.0537],\n",
       "          [ 0.0118, -0.5128,  1.3245,  ...,  0.5653, -0.3813,  0.5208]],\n",
       "\n",
       "         [[ 0.0207,  0.1167, -0.3826,  ..., -0.3049,  0.2880,  0.2964],\n",
       "          [ 0.1758, -0.0022, -0.0138,  ..., -0.1117, -0.5121, -0.3210],\n",
       "          [ 0.3615,  0.7982, -0.1853,  ...,  0.1976, -0.7275, -0.3754]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1018, -1.0599,  0.0519,  ...,  0.6631, -0.9743, -0.2237],\n",
       "          [ 0.2672, -0.5588, -0.3430,  ...,  0.3346, -0.7920,  0.4303],\n",
       "          [ 0.1788, -0.6711, -0.9735,  ...,  0.0703, -0.9365,  0.9127]],\n",
       "\n",
       "         [[ 0.1258,  0.4108,  0.0566,  ..., -0.4919, -0.1571, -0.0419],\n",
       "          [-0.5005, -0.1503,  0.3634,  ..., -0.1573,  0.4520, -0.4495],\n",
       "          [-0.2719,  0.2945,  0.2073,  ...,  0.6143,  0.3395, -0.2070]],\n",
       "\n",
       "         [[ 0.2814,  0.1630, -0.0985,  ...,  0.1265,  0.2289, -0.4706],\n",
       "          [ 0.1739, -0.4256, -0.2478,  ...,  1.3197,  0.1611, -0.5711],\n",
       "          [-0.2757, -0.1414,  0.0874,  ...,  0.5035,  0.3456,  0.1677]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-0.1429, -1.2605,  0.5155,  ..., -0.8424, -0.1206, -0.1751],\n",
       "          [ 0.4389, -1.2751,  0.1951,  ..., -0.9653,  0.0426, -0.2758],\n",
       "          [ 0.8637, -1.6461,  0.7533,  ..., -0.4251,  0.0454, -0.0657]],\n",
       "\n",
       "         [[ 0.3592, -0.0501, -0.1067,  ...,  0.3210,  0.1828, -0.0534],\n",
       "          [ 0.6264,  0.0056,  1.0558,  ..., -0.0182,  1.0566, -0.3656],\n",
       "          [-0.0065, -0.2762,  1.1811,  ..., -0.5955,  1.0374, -0.0709]],\n",
       "\n",
       "         [[-0.6117,  0.3266, -0.3508,  ..., -0.2815,  0.8657, -0.8498],\n",
       "          [-0.5603, -0.3037, -0.6139,  ..., -0.3222,  0.8605,  0.0928],\n",
       "          [-0.6923, -0.2381, -0.7513,  ..., -0.1719,  0.3120,  0.1144]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.3000, -0.0114,  0.5482,  ...,  0.6022, -0.5609, -0.3541],\n",
       "          [-0.4326, -0.4408,  0.1866,  ..., -0.2005, -1.2912, -0.2746],\n",
       "          [-0.4044, -0.0207,  0.6364,  ...,  0.4930, -0.3606, -0.0879]],\n",
       "\n",
       "         [[-0.3489, -0.3796,  0.9306,  ..., -0.2230, -0.2650,  0.2364],\n",
       "          [-0.3199, -0.2966,  0.9647,  ..., -0.4260, -0.6473,  0.1890],\n",
       "          [ 0.0424, -0.9153,  0.8675,  ..., -0.1145,  0.2331,  0.2957]],\n",
       "\n",
       "         [[-0.2643, -0.2797, -0.3701,  ..., -0.2950, -0.1995, -0.2092],\n",
       "          [ 0.0825, -0.7622, -0.4637,  ..., -0.7905, -0.1055, -0.0196],\n",
       "          [ 0.2652, -0.4922, -0.2942,  ..., -0.6206,  0.0608,  0.2562]]]],\n",
       "       grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=(tensor([[[[1.1111, 0.0000, 0.0000],\n",
       "          [0.6446, 0.4666, 0.0000],\n",
       "          [0.3353, 0.4344, 0.3413]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.5213, 0.5898, 0.0000],\n",
       "          [0.3768, 0.3103, 0.0000]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5055, 0.6056, 0.0000],\n",
       "          [0.4103, 0.2111, 0.4898]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.7183, 0.3929, 0.0000],\n",
       "          [0.6286, 0.2703, 0.2122]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5140, 0.5971, 0.0000],\n",
       "          [0.3010, 0.3699, 0.4402]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4541, 0.6570, 0.0000],\n",
       "          [0.0000, 0.3400, 0.4418]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4474, 0.0000, 0.0000],\n",
       "          [0.3421, 0.3404, 0.4286]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6802, 0.4309, 0.0000],\n",
       "          [0.2906, 0.0000, 0.3462]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4621, 0.6491, 0.0000],\n",
       "          [0.6145, 0.2697, 0.2269]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4027, 0.7084, 0.0000],\n",
       "          [0.3799, 0.3674, 0.3638]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5202, 0.5909, 0.0000],\n",
       "          [0.3840, 0.4829, 0.2442]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6063, 0.0000, 0.0000],\n",
       "          [0.3190, 0.5208, 0.2713]]]], grad_fn=<MulBackward0>), tensor([[[[1.1111, 0.0000, 0.0000],\n",
       "          [0.6654, 0.4457, 0.0000],\n",
       "          [0.4738, 0.2717, 0.3657]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5423, 0.5688, 0.0000],\n",
       "          [0.3851, 0.3281, 0.0000]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4909, 0.6202, 0.0000],\n",
       "          [0.2959, 0.4221, 0.3931]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.5933, 0.5178, 0.0000],\n",
       "          [0.3871, 0.3803, 0.3436]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6408, 0.4704, 0.0000],\n",
       "          [0.3683, 0.3667, 0.3761]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6279, 0.4832, 0.0000],\n",
       "          [0.3553, 0.3133, 0.4425]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6233, 0.4878, 0.0000],\n",
       "          [0.3579, 0.2987, 0.4545]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4911, 0.6200, 0.0000],\n",
       "          [0.3517, 0.4191, 0.3403]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.6133, 0.0000],\n",
       "          [0.0000, 0.2817, 0.4733]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.4938, 0.6173, 0.0000],\n",
       "          [0.3913, 0.3133, 0.4066]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6691, 0.4420, 0.0000],\n",
       "          [0.0000, 0.3446, 0.4237]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.4806, 0.6305, 0.0000],\n",
       "          [0.2908, 0.4048, 0.4155]]]], grad_fn=<MulBackward0>), tensor([[[[1.1111, 0.0000, 0.0000],\n",
       "          [0.6204, 0.4907, 0.0000],\n",
       "          [0.4533, 0.2933, 0.3645]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6920, 0.4191, 0.0000],\n",
       "          [0.7101, 0.1859, 0.2150]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5454, 0.5658, 0.0000],\n",
       "          [0.2980, 0.3811, 0.4320]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4588, 0.6523, 0.0000],\n",
       "          [0.0000, 0.4321, 0.3311]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5247, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.5948, 0.5163, 0.0000],\n",
       "          [0.3673, 0.3135, 0.4303]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4677, 0.6434, 0.0000],\n",
       "          [0.3064, 0.3848, 0.4199]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6688, 0.4423, 0.0000],\n",
       "          [0.4647, 0.3281, 0.3184]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.5612, 0.5499, 0.0000],\n",
       "          [0.3294, 0.4212, 0.3605]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5245, 0.0000],\n",
       "          [0.4026, 0.2965, 0.4121]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5804, 0.5307, 0.0000],\n",
       "          [0.3213, 0.3489, 0.4409]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6276, 0.4835, 0.0000],\n",
       "          [0.3083, 0.3628, 0.4401]]]], grad_fn=<MulBackward0>), tensor([[[[1.1111, 0.0000, 0.0000],\n",
       "          [0.7450, 0.3661, 0.0000],\n",
       "          [0.4086, 0.3617, 0.0000]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3882, 0.0000],\n",
       "          [0.4146, 0.2860, 0.4105]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.7116, 0.3995, 0.0000],\n",
       "          [0.5184, 0.2657, 0.3270]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6309, 0.4802, 0.0000],\n",
       "          [0.3458, 0.4633, 0.3020]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4167, 0.6944, 0.0000],\n",
       "          [0.2840, 0.4994, 0.3277]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5768, 0.5343, 0.0000],\n",
       "          [0.0000, 0.4276, 0.3561]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5360, 0.5751, 0.0000],\n",
       "          [0.3010, 0.3440, 0.4661]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.6217, 0.4895, 0.0000],\n",
       "          [0.4312, 0.3272, 0.3527]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6664, 0.4447, 0.0000],\n",
       "          [0.4149, 0.4317, 0.2645]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5249, 0.0000, 0.0000],\n",
       "          [0.4460, 0.3739, 0.2912]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.5348, 0.5763, 0.0000],\n",
       "          [0.3403, 0.3759, 0.3949]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.3613, 0.3817, 0.3681]]]], grad_fn=<MulBackward0>), tensor([[[[1.1111, 0.0000, 0.0000],\n",
       "          [0.6090, 0.5021, 0.0000],\n",
       "          [0.3973, 0.0000, 0.4186]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5101, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3452, 0.3707]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5892, 0.5219, 0.0000],\n",
       "          [0.3312, 0.4850, 0.2949]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4630, 0.0000],\n",
       "          [0.4147, 0.3144, 0.3820]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.5715, 0.5396, 0.0000],\n",
       "          [0.4674, 0.3252, 0.3185]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4769, 0.6342, 0.0000],\n",
       "          [0.3458, 0.4392, 0.3262]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6044, 0.5067, 0.0000],\n",
       "          [0.3892, 0.4100, 0.3119]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4901, 0.6210, 0.0000],\n",
       "          [0.2658, 0.4460, 0.0000]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5453, 0.5659, 0.0000],\n",
       "          [0.3763, 0.3792, 0.3555]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5148, 0.0000, 0.0000],\n",
       "          [0.3563, 0.4325, 0.3223]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6168, 0.4943, 0.0000],\n",
       "          [0.4583, 0.3542, 0.2985]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4386, 0.6725, 0.0000],\n",
       "          [0.0000, 0.3351, 0.3893]]]], grad_fn=<MulBackward0>), tensor([[[[1.1111, 0.0000, 0.0000],\n",
       "          [0.5494, 0.5617, 0.0000],\n",
       "          [0.3867, 0.3500, 0.3745]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5558, 0.5553, 0.0000],\n",
       "          [0.4488, 0.3781, 0.2842]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6354, 0.4757, 0.0000],\n",
       "          [0.3855, 0.3143, 0.4113]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6639, 0.4472, 0.0000],\n",
       "          [0.3815, 0.2841, 0.4456]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5468, 0.5643, 0.0000],\n",
       "          [0.3829, 0.4331, 0.2952]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6320, 0.0000, 0.0000],\n",
       "          [0.4168, 0.3207, 0.3736]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6030, 0.5081, 0.0000],\n",
       "          [0.4243, 0.3066, 0.3802]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4543, 0.6568, 0.0000],\n",
       "          [0.3580, 0.4683, 0.2848]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6177, 0.4934, 0.0000],\n",
       "          [0.3732, 0.3576, 0.3803]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.4504, 0.6607, 0.0000],\n",
       "          [0.3465, 0.3926, 0.3721]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4416, 0.6695, 0.0000],\n",
       "          [0.3431, 0.3487, 0.0000]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5110, 0.0000],\n",
       "          [0.3916, 0.3996, 0.3199]]]], grad_fn=<MulBackward0>), tensor([[[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5732, 0.5380, 0.0000],\n",
       "          [0.3238, 0.3855, 0.4018]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5695, 0.5416, 0.0000],\n",
       "          [0.3137, 0.3748, 0.4226]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6227, 0.4884, 0.0000],\n",
       "          [0.4312, 0.2648, 0.4151]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.7387, 0.3724, 0.0000],\n",
       "          [0.3785, 0.3832, 0.3494]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5517, 0.0000],\n",
       "          [0.4362, 0.3382, 0.3366]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5237, 0.5874, 0.0000],\n",
       "          [0.3288, 0.3937, 0.3886]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5132, 0.5979, 0.0000],\n",
       "          [0.4334, 0.3214, 0.0000]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5115, 0.0000],\n",
       "          [0.3565, 0.4095, 0.3451]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4963, 0.0000],\n",
       "          [0.4368, 0.0000, 0.4148]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5621, 0.5490, 0.0000],\n",
       "          [0.2859, 0.3507, 0.0000]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.3666, 0.3401, 0.4044]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4991, 0.6120, 0.0000],\n",
       "          [0.3746, 0.3808, 0.3558]]]], grad_fn=<MulBackward0>), tensor([[[[1.1111, 0.0000, 0.0000],\n",
       "          [0.7114, 0.3997, 0.0000],\n",
       "          [0.4660, 0.0000, 0.3467]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4998, 0.0000],\n",
       "          [0.3503, 0.4588, 0.3020]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5443, 0.5669, 0.0000],\n",
       "          [0.4404, 0.3160, 0.3546]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.5531, 0.5580, 0.0000],\n",
       "          [0.0000, 0.3455, 0.4455]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4064, 0.7047, 0.0000],\n",
       "          [0.0000, 0.4016, 0.3588]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5304, 0.5807, 0.0000],\n",
       "          [0.3616, 0.3346, 0.4149]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4967, 0.0000, 0.0000],\n",
       "          [0.3578, 0.3433, 0.4099]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5817, 0.5294, 0.0000],\n",
       "          [0.4392, 0.3105, 0.3614]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6430, 0.4681, 0.0000],\n",
       "          [0.5066, 0.2491, 0.3554]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5652, 0.5459, 0.0000],\n",
       "          [0.3308, 0.3749, 0.4055]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4563, 0.6548, 0.0000],\n",
       "          [0.0000, 0.4897, 0.3396]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5785, 0.5326, 0.0000],\n",
       "          [0.4442, 0.3539, 0.3129]]]], grad_fn=<MulBackward0>), tensor([[[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5205, 0.5906, 0.0000],\n",
       "          [0.0000, 0.3320, 0.4293]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4975, 0.6136, 0.0000],\n",
       "          [0.3090, 0.4670, 0.0000]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5358, 0.5753, 0.0000],\n",
       "          [0.3851, 0.0000, 0.3934]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.5080, 0.6031, 0.0000],\n",
       "          [0.3220, 0.4070, 0.3821]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6280, 0.4831, 0.0000],\n",
       "          [0.4111, 0.3984, 0.3016]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6594, 0.4517, 0.0000],\n",
       "          [0.4513, 0.2859, 0.3739]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.7340, 0.0000],\n",
       "          [0.2613, 0.5240, 0.3258]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5167, 0.5945, 0.0000],\n",
       "          [0.3378, 0.3823, 0.3910]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6147, 0.4964, 0.0000],\n",
       "          [0.3740, 0.4269, 0.3103]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6341, 0.4770, 0.0000],\n",
       "          [0.4444, 0.3252, 0.3416]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4066, 0.7045, 0.0000],\n",
       "          [0.3107, 0.3891, 0.4113]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6007, 0.0000, 0.0000],\n",
       "          [0.3331, 0.3619, 0.4160]]]], grad_fn=<MulBackward0>), tensor([[[[1.1111, 0.0000, 0.0000],\n",
       "          [0.6222, 0.4889, 0.0000],\n",
       "          [0.4081, 0.3150, 0.0000]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4702, 0.6409, 0.0000],\n",
       "          [0.3015, 0.0000, 0.4429]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6170, 0.4941, 0.0000],\n",
       "          [0.3717, 0.3607, 0.3787]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5532, 0.5580, 0.0000],\n",
       "          [0.3290, 0.3712, 0.4109]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6522, 0.4589, 0.0000],\n",
       "          [0.4645, 0.0000, 0.2853]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6231, 0.4880, 0.0000],\n",
       "          [0.3974, 0.3712, 0.3425]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5825, 0.5286, 0.0000],\n",
       "          [0.3231, 0.3663, 0.4217]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.4472, 0.6639, 0.0000],\n",
       "          [0.3035, 0.3622, 0.4454]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5468, 0.5643, 0.0000],\n",
       "          [0.3924, 0.4481, 0.2706]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4779, 0.6332, 0.0000],\n",
       "          [0.3378, 0.3048, 0.4685]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.7114, 0.3997, 0.0000],\n",
       "          [0.5301, 0.3467, 0.2343]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5421, 0.0000],\n",
       "          [0.2886, 0.4061, 0.4164]]]], grad_fn=<MulBackward0>), tensor([[[[0.0000, 0.0000, 0.0000],\n",
       "          [0.6375, 0.4736, 0.0000],\n",
       "          [0.4013, 0.3801, 0.3297]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5472, 0.0000, 0.0000],\n",
       "          [0.2701, 0.3772, 0.4638]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5893, 0.5218, 0.0000],\n",
       "          [0.4308, 0.3865, 0.2938]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.6517, 0.0000],\n",
       "          [0.2915, 0.4907, 0.3288]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6425, 0.4687, 0.0000],\n",
       "          [0.4736, 0.3248, 0.3127]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5664, 0.5447, 0.0000],\n",
       "          [0.3497, 0.3083, 0.4531]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5363, 0.5748, 0.0000],\n",
       "          [0.4021, 0.3696, 0.3394]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5966, 0.5145, 0.0000],\n",
       "          [0.3439, 0.3667, 0.4005]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4997, 0.6114, 0.0000],\n",
       "          [0.3762, 0.3929, 0.3420]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4897, 0.6214, 0.0000],\n",
       "          [0.3051, 0.3555, 0.4505]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6670, 0.0000, 0.0000],\n",
       "          [0.4777, 0.2780, 0.3554]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4388, 0.6723, 0.0000],\n",
       "          [0.2966, 0.4587, 0.3558]]]], grad_fn=<MulBackward0>), tensor([[[[1.1111, 0.0000, 0.0000],\n",
       "          [0.5584, 0.5527, 0.0000],\n",
       "          [0.3682, 0.4599, 0.2830]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4981, 0.6131, 0.0000],\n",
       "          [0.3301, 0.3615, 0.4195]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6254, 0.4857, 0.0000],\n",
       "          [0.3567, 0.3149, 0.4394]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5414, 0.0000],\n",
       "          [0.3353, 0.4324, 0.3434]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.3805, 0.7306, 0.0000],\n",
       "          [0.2469, 0.4235, 0.4406]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.5704, 0.5407, 0.0000],\n",
       "          [0.4321, 0.4584, 0.2207]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5311, 0.0000, 0.0000],\n",
       "          [0.2612, 0.4135, 0.4364]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5698, 0.5413, 0.0000],\n",
       "          [0.4623, 0.3899, 0.2589]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5162, 0.5949, 0.0000],\n",
       "          [0.4006, 0.4238, 0.2867]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.4956, 0.6155, 0.0000],\n",
       "          [0.3122, 0.4004, 0.3985]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.6928, 0.4183, 0.0000],\n",
       "          [0.4885, 0.3140, 0.0000]],\n",
       "\n",
       "         [[1.1111, 0.0000, 0.0000],\n",
       "          [0.5138, 0.5974, 0.0000],\n",
       "          [0.2614, 0.3810, 0.4686]]]], grad_fn=<MulBackward0>)), cross_attentions=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/mntcephfs/data/ruoyusun/common_dirs/gpt2-small')\n",
    "input = tokenizer('hello, everyone',return_tensors=\"pt\")\n",
    "\n",
    "model(**input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-11): 12 x GPT2Block_CoPE(\n",
       "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GPT2Attention_CoPE(\n",
       "      (c_attn): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (cope): CoPE()\n",
       "    )\n",
       "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
