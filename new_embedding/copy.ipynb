{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from transformers.cache_utils import Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_fun(value):\n",
    "    #初步先定为sigmoid函数\n",
    "    return torch.sigmoid(value)\n",
    "    \n",
    "    \n",
    "def adaptive_position( query:torch.Tensor, key:torch.Tensor,position_modified_ids=None)->torch.Tensor:\n",
    "    '''输入的维度应该是 \n",
    "    key: [batch, num_heads, seq_length, hidden_dim]\n",
    "    \n",
    "    in prefilling stage:\n",
    "    query: [batch, num_heads, seq_length, hidden_dim]\n",
    "    \n",
    "    in inference stage:\n",
    "    query: [batch, num_heads, 1, hidden_dim]'''\n",
    "    batch,num_heads = query.shape[0], query.shape[1]\n",
    "    if position_modified_ids == None:\n",
    "        #此时应该是预填充阶段\n",
    "        #注意要错位相乘, 因此query要去第一个, key要去最后一个\n",
    "        query = query[:,:,1:,:].unsqueeze(-2)\n",
    "        key = key[:,:,:-1,:].unsqueeze(-2)\n",
    "        distance = distance_fun(torch.matmul(query,key.transpose(-2,-1)).squeeze(-1)).transpose(-1,-2)\n",
    "        \n",
    "        #这个distance相比于真正的距离差着一个一号位, 我们都强制命令一号位的位置为0\n",
    "        distance_0 = torch.zeros((batch,num_heads,1,1),dtype=distance.dtype)\n",
    "        distance = torch.cat((distance_0,distance),dim=-1) #在一号位添加一个0\n",
    "        \n",
    "        position_modified_ids = distance.cumsum(dim=-1) #此时的输出维度是[batch,num_head, 1, seq_length]\n",
    "    \n",
    "    else:\n",
    "        #这个时候的输入一般是\"一个\"query, 我们的任务是计算新的query和倒数第二个key之间的距离.\n",
    "        \n",
    "        new_distance = distance_fun(torch.matmul(query, key[:,:,-2,:].unsqueeze(-2).transpose(-2,-1)))\n",
    "        position_modified_ids = new_distance + position_modified_ids[:,:,:,-1].unsqueeze(-2)\n",
    "    return position_modified_ids #返回的结果: 若是在pre-filling阶段, 则是[batch, num_head, 1, seq_length], 若是在推理阶段, 则是[batch, num_head,1,1]\n",
    "        \n",
    "def query_select(query_status, block_ends, key_cache, block_lst:list):#从给定的blocks中挑选出来最有用的那个\n",
    "    #query: [1,hidden_dim]\n",
    "    #block_ends: list[int] \n",
    "    #key_cache : [cache_length,hidden_dim]\n",
    "    block_num = len(block_ends)\n",
    "    scores = []\n",
    "    for i in range(1,block_num):\n",
    "        start = block_ends[i-1]\n",
    "        end = block_ends[i]\n",
    "        key_status = key_cache[start:end,:]\n",
    "        score = torch.mean((query_status@key_status.T))\n",
    "        scores.append(score)\n",
    "    max_id = torch.argmax(torch.tensor(scores))+1\n",
    "    end = block_ends[max_id]\n",
    "    return block_lst.index(end)\n",
    "\n",
    "def get_block_score(query_status,key_cache,obj_block, end_block):\n",
    "    obj_key = key_cache[obj_block[0]:obj_block[1],:] #[obj_length, hidden_dim]\n",
    "    end_query = query_status[end_block[0]:end_block[1],:] #[end_length, hidden_dim]\n",
    "    score = torch.mean(end_query@obj_key.T)\n",
    "    return score\n",
    "\n",
    "\n",
    "def block_select(query_status,key_cache,block_lst,neighbor_block_num):\n",
    "    #query_status:[seq_length, hidden_dim]\n",
    "    #key_cache : [cache_length, hidden_dim]\n",
    "    #block_lst : list[]\n",
    "    #输出为: block_list中最后一个block在neighbor_block中最关注的block\n",
    "    last_block_end = block_lst[-1]\n",
    "    last_block_start = block_lst[-2]\n",
    "    scores = []\n",
    "    if len(block_lst) <= neighbor_block_num: #说明前面的所有block都需要检查\n",
    "        block_lst = [0]+block_lst\n",
    "        for i in range(len(block_lst)-1):\n",
    "            \n",
    "            obj_block = []\n",
    "            score = get_block_score()\n",
    "    elif len(block_lst) > neighbor_block_num:\n",
    "\n",
    "\n",
    "def block_trace(block_dependency,\n",
    "                select_block_id, \n",
    "                left_length,block_lst):#这个block_dependecy是针对某个batch和head_idx的\n",
    "    #key_cache:[seq_length,hidden_dim]\n",
    "    #输入的left_length 是去除掉[skip_block]和[sink_block]以后的长度\n",
    "    select_keys_position = []\n",
    "    while block_dependency[select_block_id] != None and left_length>0: #此时说明select_block_id不是0\n",
    "        select_block_id = block_dependency[select_block_id]\n",
    "        end = block_lst[select_block_id]\n",
    "        start = block_lst[select_block_id-1]\n",
    "        select_keys_position = [[start,end]] + select_keys_position\n",
    "        left_length -= (end-start)\n",
    "    return select_keys_position\n",
    "    \n",
    "\n",
    "def query_aware(query:torch.Tensor, #[batch,num_heads, seq_length,hidden_dim]\n",
    "                key_cache, #[batch, num_heads,cache_length, hidden_dim]\n",
    "                position_ids,\n",
    "                block_status, \n",
    "                block_dependency,\n",
    "                max_length,\n",
    "                skip_num=5, #其实这个skip_block更应该叫做neighbor_block, 我懒得改了\n",
    "                find_num = 5,\n",
    "                sink_num = 1):\n",
    "    #输入的query维度是[batch, num_head, seq_length, hidden_dim]\n",
    "    #输入的block是{batch1:{head1:[...],head2:[...],...},batch2:{head1:[...],head2:[...],...},...}\n",
    "    #输入的position_ids维度是[1,seq_length]\n",
    "    #输出是字典: {batch1:{head1:[[a1,a2,...],[b1,b2,...]]}}字典嵌套, 每个head的value是列表, 列表中每个元素是query挑选出来的token_ids, 列表的长度是seq_length\n",
    "    batch_size, head_num = query.shape[:2]\n",
    "    position_min = position_ids[0,0]\n",
    "    seq_length = position_ids.shape[-1]\n",
    "    keys_to_be_used = {}\n",
    "    for batch in range(batch_size):\n",
    "        keys_to_be_used[batch] = {} #理想状态的key_cache的组合应该是: [sink_block] + [select_block] + [skip_block]\n",
    "        for head_idx in range(head_num):\n",
    "            \n",
    "            i = 0\n",
    "            block_lst = block_status[batch][head_idx]\n",
    "            element_num = len(block_lst)\n",
    "            for delta in range(seq_length):\n",
    "                query_position = position_min+delta\n",
    "                while block_lst[i] < query_position and i < element_num: #lst中第i-1个元素就是我们要找的那个, 即共有0...i-1 共计i个元素在query前面\n",
    "                    i += 1\n",
    "                if i <= skip_num: #说明我这个query前面的所有key_cache都有用, 此时没有select_block, 所有的sink_block和ship_block重合\n",
    "                    keys_to_be_used[batch][head_idx] = key_cache[batch,head_idx,:query_position,:]\n",
    "\n",
    "                else: #在这种状态下, key_cache的组合应该是: [sink_block] + [select_block] + [skip_block]\n",
    "                    end = i-skip_num \n",
    "                    if end > find_num: \n",
    "                        start = end-find_num-1\n",
    "                        #接下来的block_ends只是用来在其中根据query挑选一个最有用的出来, 使用函数query_select\n",
    "                        block_ends = block_lst[start:end] #注意, 此时len(block_ends) = end-start = find_num+1, 第一个元素是上个block的结尾\n",
    "                    else: #此时满足: 跳过了一些block, 然后剩下的block不够了\n",
    "                        block_ends = [0] + block_lst[:end]\n",
    "                    query_status = query[batch,head_idx,query_position,:]\n",
    "                    selected_block_id = query_select(query_status,block_ends, key_cache[batch,head_idx,:,:],block_lst) #注意: 返回的是block_lst中被选中的block的id\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block_Cache():\n",
    "    def __init__(self,block_size=256) -> None:\n",
    "        self.position_cache:list[torch.Tensor] = []#列表中每个元素是三维的tensor[batch, num_heads, seq_length]\n",
    "        self.block_cache = [] #列表中每个元素是一个字典嵌套字典的结构{batch:{head_idx:{[78,155,283,...]}}}\n",
    "        self.block_size = block_size\n",
    "        self.block_dependency = []#每一个元素都是一个dict{batch: {head_idx:[None, 78, 78,...]}} 其中列表的长度应该和block_cache中列表的长度相同 \n",
    "\n",
    "        \n",
    "    def get_block_token_ids(self,query_ids:torch.Tensor, block_num = 5): #输入的query形状为[batch, num_heads, 1,query_length]\n",
    "        #对应的输出维度应该是[batch, num_heads, query_length, block_num]\n",
    "        batch_size, num_heads = query_ids.shape[0:2]\n",
    "        seq_length =query_ids.shape[-1]\n",
    "        for batch in range(batch_size):\n",
    "            for head_idx in range(num_heads):\n",
    "                query_positions = query_ids[batch,head_idx]#[1,seq_length]\n",
    "\n",
    "    def update(self, \n",
    "               position_modified_ids: torch.Tensor, \n",
    "               layer_idx: int,):\n",
    "        position_modified_ids = position_modified_ids % self.block_size\n",
    "        if len(self.position_cache) <= layer_idx:\n",
    "            self.position_cache.append(position_modified_ids)\n",
    "        else:\n",
    "            self.position_cache[layer_idx] = torch.cat([self.position_cache[layer_idx], position_modified_ids], dim = -1)\n",
    "        \n",
    "        \n",
    "        batch_size, num_heads = position_modified_ids.shape[0:2]\n",
    "        seq_length = self.position_cache[layer_idx].shape[-1]\n",
    "\n",
    "        if len(self.block_cache) <= layer_idx:\n",
    "            block_record = {}\n",
    "            block_dependency = {}\n",
    "            for batch in range(batch_size):\n",
    "                block_record[batch] = {}\n",
    "                for head in range(num_heads):\n",
    "                    block_record[batch][head] = []\n",
    "                    block_dependency[batch][head] = []\n",
    "                    for i in range(seq_length-1):\n",
    "                        if position_modified_ids[batch,head,0,i] < position_modified_ids[batch,head,0,i-1]: #记录每个block的最后一个token后一个的位置, 这是为了在之后切片的时候统一计算\n",
    "                            block_record[batch][head].append(i)\n",
    "            self.block_cache.append(block_record)\n",
    "        \n",
    "        else:\n",
    "            block_record = self.block_cache[layer_idx]\n",
    "            assert seq_length == 1 , f'在推理阶段假设一次只新增一个token, 而现在新增了{seq_length}个'\n",
    "            \n",
    "            for batch in range(batch_size):\n",
    "                for head_idx in range(num_heads):\n",
    "                    position_ids = self.position_cache[layer_idx]\n",
    "                    if position_ids[batch,head_idx,0,-1] <  position_ids[batch,head_idx,0,-2]:\n",
    "                        block_record[batch][head_idx].append(seq_length-1)\n",
    "        return self.position_cache[layer_idx], self.block_cache[layer_idx]\n",
    "    \n",
    "\n",
    "    def get_status(self,layer_idx):\n",
    "        if len(self.block_cache) <= layer_idx:\n",
    "            return None\n",
    "        else: \n",
    "            return self.position_cache[layer_idx], self.block_cache[layer_idx]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
