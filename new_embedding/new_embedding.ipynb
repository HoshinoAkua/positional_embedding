{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mntcephfs/lab_data/hanyizhou/anaconda/pe/lib/python3.11/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/mntcephfs/lab_data/hanyizhou/anaconda/pe/lib/python3.11/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905969073/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from transformers.models.llama.modeling_llama import rotate_half, repeat_kv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前需要完成的事情有两件: \n",
    "\n",
    "1. 验证对于两个token1 & token2, query(token1)$\\sim$ key(token2), 则 key(token1) $\\sim$ key(token2). 进一步我希望可以有: 对任意query, 若query(token1) $\\sim$ key(token2), 且 query $\\sim$ key(token1), 那么有 query(token2), 并且这个二元关系可以远程传递不衰减.\n",
    "\n",
    "\n",
    "2. block 分块的超参数大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4eeb25f0db4c57b25c204dd36bdb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_dir = '/mntcephfs/data/ruoyusun/liziniu/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423'\n",
    "llama = LlamaForCausalLM.from_pretrained(llama_dir,attn_implementation=\"eager\")\n",
    "max_positions = 4096\n",
    "attn_bias = torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(\n",
    "                1, 1, max_positions, max_positions\n",
    "            )\n",
    "def attention_score_wo_rotary(layer_idx, hidden_states, num_heads=32, head_dim = 128):\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "    \n",
    "    attn_model = llama.model.layers[layer_idx]\n",
    "    \n",
    "    layer_norm = attn_model.input_layernorm\n",
    "    \n",
    "    hidden_states = layer_norm(hidden_states)\n",
    "    \n",
    "    query_states = attn_model.self_attn.q_proj(hidden_states)\n",
    "    key_states = attn_model.self_attn.k_proj(hidden_states)\n",
    "    value_states = attn_model.self_attn.v_proj(hidden_states)\n",
    "    \n",
    "    query_states = query_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(bsz, q_len, num_heads,head_dim).transpose(1, 2)\n",
    "    \n",
    "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(head_dim)\n",
    "    attn_shape = attn_weights.shape\n",
    "    \n",
    "    query_length, key_length = attn_shape[-2],attn_shape[-1]\n",
    "    causal_mask = attn_bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "    mask_value = torch.finfo(attn_weights.dtype).min\n",
    "    mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n",
    "    attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n",
    "    return attn_weights[0]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在推理阶段, query位置增加的速度要改变, 就像cope中keys距离增加的速度改变那样.\n",
    "比如在序列[a,b,c,d]中, 他们的位置是[0,1,2,3], 这个时候新来一个e, 则他的位置是5.\n",
    "\n",
    "\n",
    "现在对这个形式进行改变, 还是[a,b,c,d]这个序列, 我们在计算position的时候, 考虑一个映射$f:\\mathbb{R}\\to (0,1)$, 用pos(a,b)表示token a,b之间的距离, 则定义:\n",
    "$$\n",
    "pos(a,b) = f(q_b^T\\cdot k_a)\n",
    "$$,\n",
    "则他们之间的距离可以用序列$$[pos(a,b), pos(b,c), pos(c,d)]$$来表示. 那么此时计算position变为:\n",
    "\n",
    "$$\n",
    "[0,0+pos(a,b), pos(a,b)+pos(b,c), pos(a,b)+pos(b,c)+pos(c,d)]\n",
    "$$\n",
    "这个数字可以是任何数, 比如$[0.3,0.7,0.5]\\to [0,0.3, 1.0, 1.5]$. 这样在我们新添加一个新的token e, 每次只需要计算一个额外的$q^T k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n",
    "        super().__init__()\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        # For BC we register cos and sin cached\n",
    "        self.max_seq_len_cached = max_position_embeddings\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n",
    "        t = t / self.scaling_factor\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"_cos_cached\", emb.cos().to(torch.get_default_dtype()), persistent=False)\n",
    "        self.register_buffer(\"_sin_cached\", emb.sin().to(torch.get_default_dtype()), persistent=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        # Force float32 since bfloat16 loses precision on long contexts\n",
    "        # See https://github.com/huggingface/transformers/pull/29285\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "        \n",
    "def position_update(self, query, key):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
