{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.4028234663852886e+38"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.finfo(torch.tensor([1.]).dtype).min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdxPe(nn.Module):\n",
    "    def __init__(self,idx):\n",
    "        super().__init__()\n",
    "        self.idx = idx\n",
    "    def forward(self, attn_weights: torch.Tensor):\n",
    "        mask_value = torch.finfo(attn_weights.dtype).min\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mntcephfs/lab_data/hanyizhou/anaconda/pe/lib/python3.11/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/mntcephfs/lab_data/hanyizhou/anaconda/pe/lib/python3.11/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905969073/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(50257, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "dir = '/mntcephfs/data/ruoyusun/common_dirs/gpt2-small'\n",
    "model = GPT2LMHeadModel.from_pretrained(dir)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(50257, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'测试抹除第一个token的位置信息'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer, GPT2Config\n",
    "dir = '/mntcephfs/data/ruoyusun/common_dirs/gpt2-small'\n",
    "model = GPT2LMHeadModel.from_pretrained(dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(dir)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "'''测试抹除第一个token的语义信息'''\n",
    "word_embedding = model.transformer.wte.weight\n",
    "with torch.no_grad():\n",
    "    word_embedding = torch.cat((word_embedding,torch.zeros_like(word_embedding[0],requires_grad=True).reshape(1,-1)),dim=0)\n",
    "model.transformer.wte.weight = torch.nn.Parameter(word_embedding)\n",
    "'''测试抹除第一个token的位置信息'''\n",
    "# pe_embedding = model.transformer.wpe.weight\n",
    "# with torch.no_grad():\n",
    "#     pe_embedding[0] = torch.zeros_like(pe_embedding[0], requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3670, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''给token前面加上空白信息'''\n",
    "inputs_pad = tokenizer(tokenizer.pad_token+\"The decision left CNN Chief Medical Correspondent Sanjay Gupta as the only doctor at the hospital to get the patients through the night.\", return_tensors=\"pt\")\n",
    "'''两段话拼在一起的token'''\n",
    "#inputs = tokenizer(\"The decision left CNN Chief Medical Correspondent Sanjay Gupta as the only doctor at the hospital to get the patients through the night. Democratic front-runner Hillary Clinton was ahead by a slim margin in Missouri on Wednesday, but the race remained in limbo pending word on whether rival Sen. Bernie Sanders of Vermont would seek a recount.\", return_tensors=\"pt\")\n",
    "'''单一句话的token'''\n",
    "inputs = tokenizer(\"The decision left CNN Chief Medical Correspondent Sanjay Gupta as the only doctor at the hospital to get the patients through the night.\", return_tensors=\"pt\")\n",
    "#outputs_pad = model(**inputs_pad, labels=inputs_pad[\"input_ids\"], output_attentions = True)\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"], output_attentions = True, output_hidden_states=True)\n",
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "attention = [outputs.attentions[i][0].detach().cpu().numpy() for i in range(12)]\n",
    "#attention_pad = [outputs_pad.attentions[i][0].detach().cpu().numpy() for i in range(12)]\n",
    "'''画全局图像'''\n",
    "for j in range(12):\n",
    "    fig,axes = plt.subplots(ncols=12,figsize = (48,4))\n",
    "    for i in range(12):\n",
    "        axes[i].matshow(attention[j][i])\n",
    "    \n",
    "'''画一个head的6个图'''\n",
    "# head = 6\n",
    "# fig,axes = plt.subplots(ncols = 6, figsize = (36,6))\n",
    "# for i in range(6):\n",
    "#     axes[i].matshow(attention[2*i+1][head])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plt.savefig(f'./output/fig_without_semantic/fig_witout_semantic {j} layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inputs.input_ids\n",
    "embedding_layer = model.transformer.wte\n",
    "pos_layer = model.transformer.wpe\n",
    "W_matrix = []\n",
    "for i in range(12):\n",
    "    W_matrix.append(model.transformer.h[i].attn.c_attn)\n",
    "split_fun = model.transformer.h[0].attn._split_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看第一层的位置编码和token embedding的关系\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weight的2304维是768*3的结果, 分别对应q, k, v\n",
    "#W_matrix[0].weight.shape, W_matrix[0].bias.shape->(torch.Size([768, 2304]), torch.Size([2304]))\n",
    "#得到的qkv再用split_fun分割就行, 为了更清楚地知道这些矩阵的特征, 我先把他们分割\n",
    "# wq, wk, wv = [],[],[]\n",
    "# for i in range(12):\n",
    "#     weight = W_matrix[i].weight.split(768, dim=-1)\n",
    "#     bias = W_matrix[i].bias.split(768)\n",
    "#     wq.append((weight[0],bias[0]))\n",
    "#     wk.append((weight[1],bias[1]))\n",
    "#     wv.append((weight[2],bias[2]))\n",
    "\n",
    "\n",
    "'''先计算纯粹的'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于实际的计算过程中, 从 $x_i$ 变成 $q_i$ 的过程为: $q_i = W_q\\cdot (x_i+p_i)+b_q$, 做一个变形: \n",
    "$$\n",
    "q_i = W_q\\cdot x_i+b_q+W_q\\cdot p_i = (W_q\\cdot x_i+b_q)+W_q\\cdot p_i=\\tilde{q_i}+\\tilde{p_i}, \n",
    "$$\n",
    "这样我们可以分别考察:\n",
    "$\\tilde{q_i}^T\\cdot \\tilde{k_j}, \\quad\\tilde{q_i}^T\\cdot \\tilde{p_j}+\\tilde{p_i}\\cdot \\tilde{k_j},\\quad \\tilde{p_i}^T\\cdot \\tilde{p_j}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
